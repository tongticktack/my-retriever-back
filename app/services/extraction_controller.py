"""Extraction controller orchestrating friendly persona vs structured JSON extraction.

Responsibilities:
- Decide whether to run structured extraction LLM (JSON only) based on new user input & state.
- Run fast rule extraction first, then (conditionally) strict JSON LLM extraction and merge.
- Validate & normalize fields (delegates to lost_item_extractor._validate where possible).
- Determine next conversational action (ask missing field, disambiguate, confirm, or pass-through).
- Maintain per-item stages: collecting -> ready -> confirmed.
"""
from __future__ import annotations
from typing import Dict, Any, Tuple, List, Optional
import re
import json
from datetime import datetime, timedelta
from pathlib import Path

from app.services import lost_item_extractor as li_ext
from app.scripts.logging_config import get_logger
from app.domain import lost_item_schema as schema
from app.services.llm_providers import get_llm
from app.services import external_search
from config import settings

DATE_ISO_RE = re.compile(r"20\d{2}-\d{2}-\d{2}$")

PERSONA_PREFIXES: List[str] = []  # deprecated; kept for backward compatibility

# Intent keyword heuristics
SHORT_GREETING_TOKENS = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello", "ã…ã…‡"]
# Identity / capability inquiry patterns considered friendly opening, not off-topic
INTRO_PATTERNS = [
    r"ë„ˆ.*ëˆ„êµ¬", r"ë„ˆ.*ë­í•˜ëŠ”", r"ë„Œ ëˆ„êµ¬", r"ëˆ„êµ¬ì„¸ìš”", r"ì •ì²´ì„±", r"ì—­í• ì´ ë­", r"ë¬´ìŠ¨ ì—­í• ", r"ë¬´ì—‡ì„ í•  ìˆ˜", r"ë­ í•  ìˆ˜", r"ë¬´ìŠ¨ ì¼ì„", r"í•  ìˆ˜ ìˆì–´\??"
]
INTENT_LABELS = ["greeting", "policy", "item", "confirm", "cancel", "other"]
_INTENT_CACHE: Dict[str, str] = {}
_INTENT_CACHE_MAX = 500

# Toggle for final response guard LLM (lightweight style/safety polish)
ENABLE_RESPONSE_GUARD = True
# Optional stylistic enrichment (makes baseline functional message warmer & empathetic)
ENABLE_STYLE_ENRICH = True
# Allow light cute persona tone (dog helper) in enrichment
ALLOW_CUTE_PERSONA = True
# 1=ì€ì€, 2=í‘œì¤€ ê·€ì—¬ì›€, 3=ê°•ì¡° (ê³¼ì‰ ë°©ì§€ ë¡œì§ ìœ ì§€)
CUTE_TONE_LEVEL = 2
# PROMPT_VERSION now driven by environment via config.Settings (PROMPT_VERSION env var)
PROMPT_VERSION = getattr(settings, 'PROMPT_VERSION', 1)

# Vision confidence threshold for optional fields (brand/material/pattern ...)
VISION_OPTIONAL_CONF_THRESHOLD = 0.55
VISION_AUTO_OVERRIDE_THRESHOLD = 0.85  # high-confidence vision override threshold for critical fields

def _persona_wrap(prompt: str, kind: str) -> str:
    # Neutral passthrough; style handled by higher-level system prompt / guard.
    return prompt


def _classify_intent_rule(text: str) -> str:
    t = text.strip().lower()
    if not t:
        return "other"
    # short pure greeting only OR identity/intro queries
    if any(t == g.lower() or (t.startswith(g.lower()) and len(t) <= len(g)+2) for g in SHORT_GREETING_TOKENS):
        return "greeting"
    for pat in INTRO_PATTERNS:
        try:
            if re.search(pat, t):
                return "greeting"
        except Exception:
            continue
    # Lightweight item heuristics: single/short tokens that look like category/subcategory/relative date/region supplement
    hangul_only = re.sub(r"[^ê°€-í£]", "", t)
    token_count = len(re.split(r"\s+", t))
    # relative date keywords
    rel_date = any(kw in t for kw in ["ì–´ì œ","ê·¸ì œ","ê·¸ì €ê»˜","3ì¼ ì „","2ì¼ ì „","ì§€ë‚œì£¼","ì¼ì£¼ì¼ ì „"])
    # direct category/subcategory membership or typical endings (ì¹´ë“œ, ì—­, ëŒ€ì—­)
    if token_count <= 3:
        from app.domain import lost_item_schema as _schema  # local import to avoid cycle
        if any(cat == t for cat in _schema.PRIMARY_CATEGORIES):
            return "item"
        # flatten subcategories
        if any(t == sub for subs in _schema.SUBCATEGORIES.values() for sub in subs):
            return "item"
        if rel_date:
            return "item"
        # endings suggesting region (ì—­) or item type (ì¹´ë“œ, í°, ì§€ê°‘, ë…¸íŠ¸ë¶)
        if t.endswith("ì—­") and len(t) >= 3:
            return "item"
        if any(t.endswith(suf) for suf in ["ì¹´ë“œ","í°","ì§€ê°‘","ë…¸íŠ¸ë¶"]):
            return "item"
    return "other"


def _intent_cache_key(text: str) -> str:
    return text.strip().lower()[:200]


def _evict_if_needed():
    if len(_INTENT_CACHE) > _INTENT_CACHE_MAX:
        # naive eviction: drop first 50
        for i, k in enumerate(list(_INTENT_CACHE.keys())):
            del _INTENT_CACHE[k]
            if i >= 49:
                break


def _load_intent_prompt() -> str:
    fname = f"app/prompts/intent_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _classify_intent_llm(text: str) -> str:
    llm = get_llm()
    system = _load_intent_prompt() or (
        "ë„ˆëŠ” ì¸í…íŠ¸ ë¶„ë¥˜ê¸°. greeting|policy|item|confirm|cancel|other ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥."
    )
    user = f"ë©”ì‹œì§€: {text.strip()}\nì˜ ìƒê°í•˜ê³  ê°€ì¥ ì•Œë§ì€ í•˜ë‚˜ì˜ ë ˆì´ë¸”ë§Œ ì¶œë ¥."  # thinking nudge
    try:
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]).strip().lower()
    except Exception:
        return "other"
    token = re.split(r"[^a-zê°€-í£]+", out)[0] if out else "other"
    if token not in INTENT_LABELS:
        # simple containment fallback
        for lbl in INTENT_LABELS:
            if lbl in out:
                token = lbl
                break
    if token not in INTENT_LABELS:
        token = "other"
    return token


def classify_intent(text: str) -> Tuple[str, str]:
    """Return (intent, source) using rule fast-path + LLM fallback + cache."""
    key = _intent_cache_key(text)
    if key in _INTENT_CACHE:
        return _INTENT_CACHE[key], "cache"
    # confirm/cancel tokens quick
    low = key
    # Removed hard token shortcut for confirm/cancel to let LLM handle nuanced affirmation/negation.
    rule = _classify_intent_rule(text)
    if rule in {"greeting"}:
        _INTENT_CACHE[key] = rule; return rule, "rule"
    # LLM fallback for item vs other nuance
    llm_intent = _classify_intent_llm(text)
    _INTENT_CACHE[key] = llm_intent
    _evict_if_needed()
    return llm_intent, "llm"


def _rule_extract(text: str) -> Dict[str, str]:
    # Directly use internal rule analyzer (safe for controlled import)
    try:
        data = li_ext._analyze_rule(text)  # type: ignore[attr-defined]
    except Exception:
        data = {}
    return data


def _load_extraction_prompt() -> str:
    fname = f"app/prompts/extraction_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""

def _load_multimodal_prompt() -> str:
    # Prefer newest v4 taxonomy-aware prompt; fallback to v3.
    for fname in ["app/prompts/multimodal_extraction_v4.txt", "app/prompts/multimodal_extraction_v3.txt"]:
        p = Path(fname)
        if p.exists():
            try:
                return p.read_text(encoding="utf-8")
            except Exception:
                continue
    return ""


logger = get_logger("extract")

def _strict_llm_extract(user_text: str, current: Dict[str, str], image_urls: Optional[List[str]] = None) -> Dict[str, Any]:  # pragma: no cover (LLM)
    """LLM JSON-only structured extraction using external prompt file with retry & light repair."""
    llm = get_llm()
    # debug: record provider + model for diagnostics
    try:
        logger.info("llm.provider model=%s provider=%s text_len=%d images=%d", getattr(llm,'model',None), getattr(llm,'name',None), len(user_text), len(image_urls) if image_urls else 0)
    except Exception:
        pass
    today = datetime.now().date().isoformat()
    # dynamic few-shot date replacements
    try:
        yesterday = (datetime.now().date() - timedelta(days=1)).isoformat()
        d3 = (datetime.now().date() - timedelta(days=3)).isoformat()
    except Exception:
        yesterday = today
        d3 = today
    # Choose prompt: multimodal if images present else text-only
    if image_urls:
        prompt_tmpl = _load_multimodal_prompt() or _load_extraction_prompt()
    else:
        prompt_tmpl = _load_extraction_prompt()
    current_json = json.dumps(current, ensure_ascii=False) if current else "{}"
    if prompt_tmpl:
        placeholders = {
            'TODAY': today,
            'YESTERDAY': yesterday,
            'D3': d3,
            'D2': (datetime.now().date() - timedelta(days=2)).isoformat(),
            'D7': (datetime.now().date() - timedelta(days=7)).isoformat(),
            'CATEGORIES': ", ".join(schema.PRIMARY_CATEGORIES),
            'CURRENT_JSON': current_json,
            'USER_TEXT': user_text.strip(),
            'IMAGE_COUNT': str(len(image_urls) if image_urls else 0),
        }
        system = prompt_tmpl
        for k, v in placeholders.items():
            system = system.replace('{'+k+'}', v)
    else:
        system = "ë‹¨ì¼ JSON {category, subcategory, lost_date, region}; ì—†ëŠ” ê°’ í‚¤ ìƒëµ; today=" + today
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": system}
    ]
    # Vision ì§€ì› (OpenAI 4o-mini ë“±) - provider ê°€ openai ì´ê³  ì´ë¯¸ì§€ URL ì´ ìˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ parts êµ¬ì„±
    llm_name = getattr(llm, 'name', '')
    if image_urls and llm_name == 'openai':
        parts: List[Dict[str, Any]] = []
        if user_text.strip():
            parts.append({"type": "text", "text": user_text.strip()})
        for url in image_urls[:3]:  # safety cap
            parts.append({"type": "image_url", "image_url": {"url": url}})
        # ì´ë¯¸ì§€ë§Œ ìˆëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ìµœì†Œ í•œê°œì˜ text guidance ì¶”ê°€ (ì¶”ì¶œ ì§€ì‹œ)
        if not user_text.strip():
            parts.insert(0, {"type": "text", "text": "ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ë¶„ì‹¤ë¬¼ ì •ë³´ë¥¼ category, subcategory, lost_date(ëª¨ë¥´ë©´ ë¹ˆì¹¸), region(ëª¨ë¥´ë©´ ë¹ˆì¹¸) JSON ì¶”ì¶œ"})
        messages.append({"role": "user", "content": parts})
    else:
        messages.append({"role": "user", "content": user_text.strip()})

    def _attempt_parse(raw: str) -> Dict[str, str] | None:
        snippet = raw.strip()
        if '```' in snippet:
            parts = snippet.split('```')
            # choose middle part if possible
            for p in parts:
                if '{' in p and '}' in p:
                    snippet = p; break
        snippet = snippet.strip()
        snippet = re.sub(r'^json\n', '', snippet, flags=re.IGNORECASE).strip()
        # direct parse
        try:
            parsed = json.loads(snippet)
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            pass
        # salvage braces
        m = re.search(r"\{.*?\}", raw, flags=re.DOTALL)
        if m:
            try:
                parsed = json.loads(m.group(0))
                if isinstance(parsed, dict):
                    return parsed
            except Exception:
                pass
        return None

    def _clean(parsed: Dict[str, Any]) -> Dict[str, Any]:
        cleaned: Dict[str, Any] = {}
        scalar_keys = ["category", "subcategory", "lost_date", "region", "brand", "material", "pattern"]
        for k in scalar_keys:
            v = parsed.get(k)
            if isinstance(v, str):
                t = v.strip()
                if t:
                    cleaned[k] = t[:50]
        # Arrays
        def _norm_list(vals, limit):
            out = []
            if isinstance(vals, list):
                for x in vals:
                    if not isinstance(x, str):
                        continue
                    s = x.strip()
                    if not s:
                        continue
                    if len(s) > 25:
                        s = s[:25]
                    if s not in out:
                        out.append(s)
                    if len(out) >= limit:
                        break
            return out
        nf = _norm_list(parsed.get('notable_features'), 5)
        if nf:
            cleaned['notable_features'] = nf
        ts = _norm_list(parsed.get('text_snippets'), 4)
        # rudimentary PII filter: remove email-like / long digit sequences
        filtered_ts = []
        for s in ts:
            if '@' in s:
                continue
            if sum(c.isdigit() for c in s) >= 6:
                continue
            filtered_ts.append(s)
        if filtered_ts:
            cleaned['text_snippets'] = filtered_ts
        # confidences
        confidences = parsed.get('confidences')
        kept_conf: Dict[str, float] = {}
        if isinstance(confidences, dict):
            for k, v in confidences.items():
                try:
                    f = float(v)
                except Exception:
                    continue
                if f < 0 or f > 1:
                    continue
                kept_conf[k] = round(f, 2)
        # Apply threshold removal for optional fields
        for opt_key in ["brand", "material", "pattern"]:
            if opt_key in cleaned:
                conf_v = kept_conf.get(opt_key)
                if conf_v is not None and conf_v < VISION_OPTIONAL_CONF_THRESHOLD:
                    # drop field & its confidence entry
                    cleaned.pop(opt_key, None)
                    kept_conf.pop(opt_key, None)
        if kept_conf:
            cleaned['confidences'] = kept_conf
        # Normalize lost_date if present in alternative formats (YYYYMMDD, YYYY.MM.DD, YYYY/MM/DD, YYYYë…„MMì›”DDì¼)
        ld = cleaned.get('lost_date')
        if ld:
            orig = ld
            norm = None
            try:
                import re as _re
                from datetime import datetime as _dt
                p1 = _re.match(r'^(20\d{2})(\d{2})(\d{2})$', ld)
                p2 = _re.match(r'^(20\d{2})[./](\d{1,2})[./](\d{1,2})$', ld)
                p3 = _re.match(r'^(20\d{2})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼$', ld)
                if p1:
                    y, m, d = p1.groups()
                elif p2:
                    y, m, d = p2.groups()
                elif p3:
                    y, m, d = p3.groups()
                else:
                    # already maybe YYYY-MM-DD
                    if _re.match(r'^20\d{2}-\d{2}-\d{2}$', ld):
                        y, m, d = ld.split('-')
                    else:
                        y = m = d = None
                if y and m and d:
                    dt_obj = _dt(int(y), int(m), int(d))
                    norm = dt_obj.strftime('%Y-%m-%d')
            except Exception:
                norm = None
            if norm:
                cleaned['lost_date'] = norm
            else:
                # Drop if cannot normalize to canonical form
                if not re.match(r'^20\d{2}-\d{2}-\d{2}$', ld):
                    cleaned.pop('lost_date', None)
                    if 'confidences' in cleaned and 'lost_date' in cleaned['confidences']:
                        cleaned['confidences'].pop('lost_date', None)
            if orig != cleaned.get('lost_date'):
                logger.debug("normalize_date raw=%s -> %s", orig, cleaned.get('lost_date'))
        return cleaned

    raw = ""
    for attempt in range(2):  # one retry if first malformed
        try:
            raw = llm.generate(messages).strip()
            if not raw:
                logger.warning("empty_output attempt=%d", attempt)
        except Exception:
            logger.error("llm_generate_exception attempt=%d", attempt)
            return {}
        parsed = _attempt_parse(raw)
        if parsed is not None:
            return _clean(parsed)
        # repair hint for retry: force user to output ONLY JSON
        messages.append({"role": "user", "content": "JSON í˜•ì‹ ì˜¤ë¥˜. ìˆœìˆ˜ JSON ê°ì²´ë§Œ ë‹¤ì‹œ ì¶œë ¥."})
    # final debug
    logger.error("parse_failed snippet=%s", raw[:120])
    return {}


def _merge_extracted(base: Dict[str, str], new: Dict[str, str], *, allow_override: bool = False, override_keys: Optional[List[str]] = None) -> Dict[str, str]:
    """Merge newly extracted fields into base.

    Default: non-destructive (first value wins) to avoid oscillation.
    If allow_override=True (e.g. explicit user correction at confirmation stage),
    then keys in override_keys (or all keys if None) can be overwritten when new has a non-empty differing value.
    """
    if not new:
        return base
    if allow_override:
        if override_keys is None:
            override_keys = list(new.keys())
        for k, v in new.items():
            if not isinstance(v, str):
                continue
            if k in override_keys and v and v.strip():
                # overwrite only if different
                if base.get(k) != v:
                    base[k] = v
        # Ensure we still add missing keys (if override_keys limited)
        for k, v in new.items():
            if k not in base and isinstance(v, str):
                base[k] = v
    else:
        for k, v in new.items():
            if k not in base and isinstance(v, str):
                base[k] = v
    return base


# _has_ambiguity: removed (was unused after logic refactor)


CONFIRM_TOKENS: set[str] = set()  # deprecated
CANCEL_TOKENS: set[str] = set()
OFF_TOPIC_KEYWORDS = [
    "ë‚ ì”¨", "ì£¼ì‹", "íˆ¬ì", "ì½”ì¸", "ì•”í˜¸í™”í", "ê±´ê°•", "ë³‘ì›", "ì§„ë£Œ", "ì‹ë‹¨", "ì•„ì¹¨ì— ë­", "ìš”ë¦¬", "ë ˆì‹œí”¼",
    "ìš´ë™", "ë‹¤ì´ì–´íŠ¸", "ê³µë¶€", "ì‹œí—˜", "ì—¬í–‰", "í˜¸í…”"
]
SOFT_LOCK_THRESHOLD = 3


def should_run_extraction(item: Dict[str, Any], user_text: str) -> bool:
    """Always run extraction in collecting/ready unless the message is a pure confirm/cancel token.
    This favors accuracy over minimal LLM calls.
    """
    stage = item.get("stage")
    if stage not in {"collecting", "ready"}:
        return False
    # Always run extraction; confirmation now inferred at higher layer (LLM intent), not by raw tokens.
    _log_metric('extraction.trigger', reason='always')
    return True


def process_message(user_text: str, lost_state: Dict[str, Any], start_new: bool, image_urls: Optional[List[str]] = None) -> Tuple[str | None, Dict[str, Any], str, Dict[str, Any] | None]:
    """Process user message relative to lost item flow.
    Returns (assistant_reply_or_none, updated_lost_state, model_label, active_item_snapshot).
    If assistant_reply_or_none is None the caller may proceed with general LLM chat.
    """
    if lost_state is None:
        lost_state = {"items": [], "active_index": None}

    # NOTE: image-only ë©”ì‹œì§€ë¥¼ stage/context ê¸°ë°˜ìœ¼ë¡œ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
    # (1) ì•„ì§ í™œì„± ì•„ì´í…œì´ ì—†ìœ¼ë©´ ì•„ë˜ì—ì„œ ìƒˆ ì•„ì´í…œ ìƒì„± í›„ collecting íë¦„ì—ì„œ ì•ˆë‚´
    # (2) ì´ë¯¸ collecting ë‹¨ê³„ë¼ë©´ ì •ë³´ ìš”ì²­ í”„ë¡¬í”„íŠ¸
    # (3) ready/confirmed ë‹¨ê³„ë¼ë©´ ì¬ìš”ì²­ ì—†ì´ ê°„ë‹¨ í™•ì¸ ë©˜íŠ¸
    image_only_deferred = False  # ready íŒë‹¨ í›„ ì²˜ë¦¬ ìœ„í•´ í”Œë˜ê·¸
    if image_urls and not user_text.strip():
        # active item ì—¬ë¶€ / stage í™•ì¸ì€ ì•„ì´í…œ í™•ë³´ í›„ ìˆ˜í–‰ (ì¡°ê¸ˆ ë’¤)
        image_only_deferred = True

    if start_new or not lost_state.get("items"):
        lost_state["items"].append({"extracted": {}, "missing": li_ext.compute_missing({}), "stage": "collecting"})
        lost_state["active_index"] = len(lost_state["items"]) - 1

    idx = lost_state.get("active_index")
    if idx is None:
        return None, lost_state, "lost-item-flow.v2", None
    current = lost_state["items"][idx]
    user_lower = user_text.strip().lower()

    # image-only (deferred) ì²˜ë¦¬: stage/context ê¸°ë°˜ ì‘ë‹µ ê²°ì •
    if image_only_deferred:
        stage = current.get("stage")
        extracted = current.get("extracted", {})
        # í•µì‹¬ í•„ë“œ ìµœì†Œ í•œ ê°œë¼ë„ í™•ë³´ ì—¬ë¶€ (category/region/lost_date ì¤‘)
        has_core = any(extracted.get(k) for k in ["category", "region", "lost_date"]) or stage in {"ready", "confirmed"}
        if not has_core:
            reply = _persona_wrap(
                "ì´ë¯¸ì§€ë¥¼ í™•ì¸í–ˆì–´ìš”! í•˜ì§€ë§Œ ì´ë¯¸ì§€ë§Œìœ¼ë¡œëŠ” ì •í™•í•œ ê²€ìƒ‰ì´ ì–´ë ¤ì›Œìš”. "
                "ì–´ë–¤ ë¬¼ê±´ì„ ì–¸ì œ, ì–´ë””ì„œ ìƒì–´ë²„ë ¸ëŠ”ì§€ í…ìŠ¤íŠ¸ë¡œ ì•Œë ¤ì£¼ì‹œë©´ "
                "ì´ë¯¸ì§€ì™€ í•¨ê»˜ ë” ì •í™•í•œ ê²€ìƒ‰ì„ ë„ì™€ë“œë¦´ê²Œìš”. "
                "ì˜ˆ: 'ì–´ì œ ì„±ê· ê´€ëŒ€ì—ì„œ ê²€ì • ì§€ê°‘ ìƒì–´ë²„ë ¸ì–´ìš”'",
                'ask'
            )
            return reply, lost_state, "image-only-prompt", _snapshot(idx, current)
        else:
            # ì´ë¯¸ í•µì‹¬ ì •ë³´ ì¡´ì¬ â†’ ê°„ë‹¨ í™•ì¸ ë©˜íŠ¸ (stage ìœ ì§€)
            tone = 'confirm' if stage == 'ready' else 'ask'
            msg = "ì´ë¯¸ì§€ ì˜ ë°›ì•˜ì–´ìš”. ê¸°ì¡´ ë¶„ì‹¤ë¬¼ ì •ë³´ì— ì‚¬ì§„ì„ ì¶”ê°€í•´ ë‘ì—ˆìŠµë‹ˆë‹¤. í•„ìš”í•œ ìˆ˜ì •ì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì§„í–‰ ì›í•˜ì‹œë©´ 'í™•ì¸'ì´ë¼ê³  í•´ì£¼ì‹œë©´ ë¼ìš”."
            return _persona_wrap(msg, tone), lost_state, "image-ack", _snapshot(idx, current)

    # Intent detection BEFORE extraction (greeting/policy may bypass item flow)
    intent, intent_source = classify_intent(user_text)
    _log_metric('intent.classified', intent=intent, source=intent_source)
    # Soft-lock handling: if previously locked and user still off-topic, short circuit
    if lost_state.get("soft_lock") and intent not in {"item", "greeting"}:
        reply = _persona_wrap(
            "ì„œë¹„ìŠ¤ê°€ ë¶„ì‹¤ë¬¼ íšŒìˆ˜ ì§€ì› ëª¨ë“œë¡œ ì ì‹œ ì ê²¼ì–´ìš”. ë¶„ì‹¤í•œ ë¬¼ê±´ì˜ ì¢…ë¥˜, ì¥ì†Œ, ë‚ ì§œ ì¤‘ í•˜ë‚˜ë¼ë„ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ì‹œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.",
            'ask'
        )
        return reply, lost_state, "intent:locked", _snapshot(idx, current)
    # Allow greeting to release soft lock naturally
    if lost_state.get("soft_lock") and intent == 'greeting':
        lost_state.pop("soft_lock", None)

    # Ready-stage filler/image handling: ì‚¬ìš©ìê°€ ì´ë¯¸ í•„ìˆ˜ ì •ë³´ë¥¼ ì¤€ ë’¤ ì´ë¯¸ì§€ë§Œ ì¶”ê°€í•˜ê±°ë‚˜ ì§§ì€ ì¶”ì„ìƒˆ("ì—¬ê¹„ì–´", "ì‘", "ã…‡ã…‹")ë¥¼ ë³´ë‚¸ ê²½ìš° off-topic ë¡œ ê°„ì£¼í•˜ì§€ ì•Šê³  í™•ì¸ ìš”ì•½ ì¬í‘œì‹œ
    if current.get("stage") == "ready" and intent == "other":
        filler_tokens = {"ì—¬ê¹„ì–´","ì—¬ê¸°ì•¼","ì‘","ã…‡ã…‡","ã…‡ã…‹","ì˜¤ì¼€ì´","ì¢‹ì•„","ë„µ","ë„¹","í™•ì¸","ëì–´","ëìŠµë‹ˆë‹¤","ì¶”ê°€","ì‚¬ì§„","ì´ë¯¸ì§€"}
        norm = user_text.strip().lower()
        # í•œê¸€ ìëª¨ ì œê±° ê°„ë‹¨í™”
        if (not user_text.strip()) or (len(norm) <= 6 and any(tok in norm for tok in filler_tokens)) or image_urls:
            # ì¬í™•ì¸ ìš”ì•½ ì¬ì „ì†¡ (stage ìœ ì§€)
            confirmation = _build_confirmation_summary(current.get("extracted", {}))
            reply = _persona_wrap(confirmation + "\n(ì´ë¯¸ì§€/ì¶”ì„ìƒˆ ì…ë ¥ìœ¼ë¡œ ì •ë³´ë¥¼ ë³€ê²½í•˜ì§€ ì•Šì•˜ì–´ìš”. ê´œì°®ìœ¼ë©´ ê³„ì† ì§„í–‰ ì˜ì‚¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•˜ê±°ë‚˜ 'í™•ì¸'ì´ë¼ í•´ ì£¼ì„¸ìš”.)", 'confirm')
            snapshot = _snapshot(idx, current)
            if ENABLE_STYLE_ENRICH:
                enriched = _style_enrich(reply, intent="item", item_snapshot=snapshot)
                if enriched:
                    reply = enriched
            if ENABLE_RESPONSE_GUARD:
                guarded = _guard_response(reply, intent="item", model_label="lost-item-flow.v2", item_snapshot=snapshot)
                if guarded:
                    reply = guarded
            return reply, lost_state, "lost-item-flow.v2", snapshot

    # ê°„ë‹¨ í™•ì • ìì—°ì–´ (í™•ì¸, ë§ì•„ìš” ë“±) -> confirm ìœ¼ë¡œ ìŠ¹ê²©
    if current.get("stage") == "ready" and intent == "other":
        low = user_text.strip().lower()
        confirm_syns = ["í™•ì¸", "ë§ì•„", "ë§ì•„ìš”", "ë§ìŠµë‹ˆë‹¤", "ê·¸ë˜", "ê·¸ë ‡ë‹¤", "ok", "okay", "ì˜¤ì¼€ì´", "ì¢‹ì•„", "ì¢‹ìŠµë‹ˆë‹¤"]
        if any(s in low for s in confirm_syns) and 2 <= len(low) <= 15:
            intent = 'confirm'

    # ì˜ëª»ëœ confirm ì˜¤íƒ ë°©ì–´: ì‚¬ì§„ ì¶”ê°€/ì—…ë¡œë“œ ë¬¸ì˜ëŠ” confirm ì•„ë‹˜
    if current.get("stage") == "ready" and intent == 'confirm':
        low = user_text.strip().lower()
        # ë¬¼ìŒí‘œ í¬í•¨ + ì‚¬ì§„ ê´€ë ¨ ë™ì‚¬/ëª…ì‚¬ ì¡´ì¬ -> confirm ì·¨ì†Œ
        photo_words = ["ì‚¬ì§„","ì´ë¯¸ì§€","ì²¨ë¶€","ì¶”ê°€","ì˜¬ë ¤","ì—…ë¡œë“œ"]
        if ("?" in low or "?" in user_text) and any(w in low for w in photo_words):
            # intent ë¥¼ other ë¡œ ê°•ë“±í•˜ê³  ì•ˆë‚´ ì²˜ë¦¬ (filler ë¶„ê¸° ì¬ì‚¬ìš©)
            intent = 'other'
            confirmation = _build_confirmation_summary(current.get("extracted", {}))
            advise = "\nì‚¬ì§„ì€ ìµœëŒ€ 3ì¥ê¹Œì§€ ë°”ë¡œ ì˜¬ë¦¬ì…”ë„ ë¼ìš”. ì´ë¯¸ í™•ì • ì „ ë‹¨ê³„ì´ë‹ˆ ì‚¬ì§„ì„ ì¶”ê°€í•˜ê±°ë‚˜ 'í™•ì¸'ì´ë¼ê³  ë§ì”€ì£¼ì‹œë©´ ê²€ìƒ‰ì„ ì§„í–‰í• ê²Œìš”."
            reply = _persona_wrap(confirmation + advise, 'confirm')
            snapshot = _snapshot(idx, current)
            if ENABLE_STYLE_ENRICH:
                enriched = _style_enrich(reply, intent="item", item_snapshot=snapshot)
                if enriched:
                    reply = enriched
            if ENABLE_RESPONSE_GUARD:
                guarded = _guard_response(reply, intent="item", model_label="lost-item-flow.v2", item_snapshot=snapshot)
                if guarded:
                    reply = guarded
            return reply, lost_state, "lost-item-flow.v2", snapshot

    if intent in {"greeting", "policy", "other"}:
        if intent == "greeting":
            # Pass through: allow general LLM (system prompt) to craft intro instead of fixed template
            return None, lost_state, "intent:greeting-pass", _snapshot(idx, current)
        if intent == "policy":
            reply = (
                "ë¶„ì‹¤ë¬¼ì€ ë°œê²¬ í›„ ê²½ì°°/ìœ ì‹¤ë¬¼ ì„¼í„°ì— ì¸ê³„ë˜ë©´ í†µìƒ ì¼ì • ê¸°ê°„(ì˜ˆ: 6ê°œì›” ì „í›„, í’ˆëª©/ë²•ê·œì— ë”°ë¼ ì°¨ì´) ë³´ê´€ í›„ ì²˜ë¦¬ë©ë‹ˆë‹¤. "
                "ì •í™•í•œ ìµœì‹  ë³´ê´€ ê¸°ê°„ì€ ê´€í•  ê²½ì°°ì²­/ì§€ìì²´ ê³µì§€ ë˜ëŠ” ê³µì‹ ìœ ì‹¤ë¬¼ í†µí•©í¬í„¸ì„ í™•ì¸í•´ ì£¼ì„¸ìš”. \n"
                "ë¬¼ê±´ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ êµ¬ì¡°í™”í•´ì„œ ê²€ìƒ‰ ì¤€ë¹„ë¥¼ ë„ì™€ë“œë¦´ê²Œìš”."
            )
            return _persona_wrap(reply, 'ask'), lost_state, "intent:policy", _snapshot(idx, current)
        # other (off-topic or filler): allow LIMITED free-form LLM on first benign occurrence
        lowered = user_text.lower()
        keyword_hit = next((kw for kw in OFF_TOPIC_KEYWORDS if kw in lowered), None)
        count = lost_state.get("off_topic_count", 0) + 1
        lost_state["off_topic_count"] = count
        # Hard redirect / lock path first
        if count > SOFT_LOCK_THRESHOLD:
            lost_state["soft_lock"] = True
            _log_metric('intent.soft_lock', count=count)
            msg = "ë¶„ì‹¤ë¬¼ ì •ë³´ ì—†ì´ ë‹¤ë¥¸ ì£¼ì œ ëŒ€í™”ê°€ ê³„ì†ë˜ì–´ ì ì‹œ ì ê¸ˆ ìƒíƒœì…ë‹ˆë‹¤. ì˜ˆ: 'ì–´ì œ ê°•ë‚¨ì—ì„œ ê²€ì • ë°±íŒ© ìƒì–´ë²„ë ¸ì–´ìš”' ì²˜ëŸ¼ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ì‹œ ë„ì™€ë“œë¦´ê²Œìš”."
            return _persona_wrap(msg, 'ask'), lost_state, "intent:redirect-lock", _snapshot(idx, current)
        # First occurrence & no strong off-topic keyword -> pass-through to general LLM (persona prompt) with guidance flag
        if count == 1 and not keyword_hit:
            lost_state["other_llm_redirect"] = True  # prompt_builder will inject guidance system message
            return None, lost_state, "intent:other-pass", _snapshot(idx, current)
        # Subsequent occurrences or keyword hit -> templated redirect (still persona wrapped)
        if keyword_hit:
            msg = f"í•´ë‹¹ ì£¼ì œ(ì˜ˆ: {keyword_hit})ëŠ” ì´ ì„œë¹„ìŠ¤ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ìš”. ë¶„ì‹¤í•œ ë¬¼ê±´ì˜ ì¢…ë¥˜ì™€ ì¥ì†Œ, ë‚ ì§œ ì¤‘ í•˜ë‚˜ë¼ë„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”. ì˜ˆ: '3ì¼ ì „ ì‹ ì´Œì—ì„œ íŒŒë€ íœ´ëŒ€í° ìƒì–´ë²„ë ¸ì–´ìš”'"
        elif count == 2:
            msg = ("ë¨¼ì € ë¶„ì‹¤ë¬¼ ê¸°ë³¸ ì •ë³´(ì¹´í…Œê³ ë¦¬/ë‚ ì§œ/ì¥ì†Œ)ê°€ í•„ìš”í•´ìš”. ì˜ˆ: 'ì§€ë‚œì£¼ ì‹ ì´Œì—ì„œ ë°±íŒ© ìƒì–´ë²„ë ¸ì–´ìš”' ì²˜ëŸ¼ ì•Œë ¤ì£¼ì„¸ìš”.")
        else:
            msg = "ë¶„ì‹¤ë¬¼ ì •ë³´(ì˜ˆ: ì–¸ì œ, ì–´ë””ì„œ, ì–´ë–¤ ë¬¼ê±´)ë¥¼ ì…ë ¥í•´ì£¼ì…”ì•¼ ê³„ì† ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”."
        return _persona_wrap(msg, 'ask'), lost_state, "intent:redirect", _snapshot(idx, current)

    # confirm/cancel intents outside ready stage â†’ treat politely as guidance
    if intent == 'confirm' and current.get('stage') != 'ready':
        # Provide guidance on missing fields
        missing = current.get('missing') or li_ext.compute_missing(current.get('extracted', {}))
        if missing:
            base = li_ext.build_missing_field_prompt(current.get('extracted', {}), missing)
            return _persona_wrap("ì•„ì§ í™•ì¸ ì „ì´ì—ìš”. " + base, 'ask'), lost_state, 'intent:confirm-misplaced', _snapshot(idx, current)
    if intent == 'cancel' and current.get('stage') != 'ready':
        # Just encourage providing info
        return _persona_wrap("ì§„í–‰ì„ ì·¨ì†Œí•  ë‹¨ê³„ê°€ ì•„ì§ ì•„ë‹ˆì—ìš”. ë¬¼ê±´ ì •ë³´ë¥¼ ì¡°ê¸ˆ ë” ì•Œë ¤ì£¼ì‹¤ë˜ìš”?", 'ask'), lost_state, 'intent:cancel-misplaced', _snapshot(idx, current)

    # Confirmation handling (only when intent=item)
    if current.get("stage") == "ready" and intent == 'confirm':
        current["stage"] = "confirmed"
        snapshot = _snapshot(idx, current)
        # Run approximate external search now (after explicit user confirm)
        # External approximate search (results saved to state only; reply gives dynamic high-level feedback)
        extracted_payload = current.get("extracted") or {}
        search_error = False
        matches = None
        try:
            matches = external_search.approximate_external_matches(
                extracted_payload,
                place_query=extracted_payload.get('region'),
                max_results=5
            )
            if matches:
                current['external_matches'] = matches
        except Exception as e:
            logger.error('external_search confirm-stage search failed err=%s', e)
            search_error = True

        match_count = len(matches) if matches else 0
        if search_error:
            search_msg = (
                "ê²€ìƒ‰ ì¤‘ ì ì‹œ ë¬¸ì œê°€ ìˆì—ˆì–´ìš”. ì¡°ê¸ˆ ë’¤ ë‹¤ì‹œ ì‹œë„í•´ ë³¼ê²Œìš”. ì¼ë‹¨ ì¶”ì¶œëœ ì •ë³´ëŠ” ì €ì¥í•´ ë‘ì—ˆì–´ìš”. "
                "í˜¹ì‹œ ì¶”ê°€ ë‹¨ì„œ(ìƒ‰ìƒ, íŠ¹ì§•, ë” ì •í™•í•œ ì¥ì†Œ)ê°€ ìˆë‹¤ë©´ ë§ì”€í•´ì£¼ì„¸ìš”!"
            )
        elif match_count == 0:
            search_msg = (
                "ì•„ì§ ë°”ë¡œ ë– ì˜¤ë¥´ëŠ” ê³µê°œ ìŠµë“ë¬¼ í›„ë³´ëŠ” ì—†ì–´ìš”. "
                "ì¡°ê¸ˆ ë” êµ¬ì²´ì ì¸ íŠ¹ì§•(ìƒ‰ìƒ, ë¸Œëœë“œ, ì„¸ë¶€ ì¥ì†Œ)ì„ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ì‹œ ë” ì˜ ì°¾ì•„ë³¼ê²Œìš”!"
            )
        elif match_count == 1:
            search_msg = (
                "ë£¨ì‹œê°€ ê°€ëŠ¥í•œ í›„ë³´ 1ê°œë¥¼ ì°¾ì•„ì™”ì–´ìš” ë©ë©! í™”ë©´ì— ë³´ì´ëŠ” í›„ë³´ê°€ ë§ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”. "
                "ë‹¤ë¥¸ ë‹¨ì„œê°€ ìˆìœ¼ë©´ ë” ì•Œë ¤ì£¼ì„¸ìš”."
            )
        elif match_count <= 3:
            search_msg = (
                f"ë£¨ì‹œê°€ ìœ ë ¥í•œ í›„ë³´ {match_count}ê°œë¥¼ ë¬¼ê³  ì™”ì–´ìš” ë©ë©! í™”ë©´ì˜ ëª©ë¡ì„ í™•ì¸í•´ ì£¼ì„¸ìš”. "
                "ë§ëŠ” ê²ƒì´ ì—†ë‹¤ë©´ ì¶”ê°€ íŠ¹ì§•ì„ ë§ì”€í•´ ì£¼ì‹œë©´ ë” ì¢í˜€ë³¼ê²Œìš”."
            )
        else:
            search_msg = (
                f"ë£¨ì‹œê°€ ê´€ë ¨ ìˆì–´ ë³´ì´ëŠ” í›„ë³´ {match_count}ê°œë¥¼ ì°¾ì•˜ì–´ìš”. ìƒìœ„ ì¼ë¶€ë§Œ ë¨¼ì € ë³´ì—¬ë“œë ¸ì–´ìš”. "
                "ë” ì„¸ë°€í•œ ë‹¨ì„œë¥¼ ì£¼ì‹œë©´ ê²°ê³¼ë¥¼ ë” ì •ë°€í•˜ê²Œ ì¢í ìˆ˜ ìˆì–´ìš”!"
            )
        search_msg += "\nìƒˆ ë¶„ì‹¤ë¬¼ì´ ìˆë‹¤ë©´ ë°”ë¡œ ìƒˆ ì„¤ëª…ì„ ì‹œì‘í•´ ì£¼ì‹œë©´ ë¼ìš” (íŠ¹ë³„í•œ ëª…ë ¹ì–´ í•„ìš” ì—†ìŒ)."
        snapshot = _snapshot(idx, current)
        return (search_msg, lost_state, "lost-item-flow.v2", snapshot)
    if current.get("stage") == "ready" and intent == 'cancel':
        current["stage"] = "collecting"
        # fall through to ask again

    # If confirmed and user gives more info without start_new trigger -> start a new item
    if current.get("stage") == "confirmed" and not start_new:
        lost_state["items"].append({"extracted": {}, "missing": li_ext.compute_missing({}), "stage": "collecting"})
        lost_state["active_index"] = len(lost_state["items"]) - 1
        idx = lost_state["active_index"]
        current = lost_state["items"][idx]

    # If previously soft-locked and now user provides item intent, unlock
    if lost_state.get("soft_lock") and intent == 'item':
        lost_state.pop("soft_lock", None)
        _log_metric('intent.soft_lock_release')

    # Extraction decision
    prev_extracted = current.get("extracted", {}).copy()
    prev_missing = current.get("missing", [])
    if should_run_extraction(current, user_text):
        extracted = current.get("extracted", {}).copy()
        # Rule first
        rule_part = _rule_extract(user_text)
        _merge_extracted(extracted, rule_part)
        # Always call strict LLM for potential refinement (now vision-aware)
        missing_before = li_ext.compute_missing(extracted)
        _log_metric('extraction.llm_call', missing=len(missing_before), ambiguity='1' if 'lost_date_candidates' in extracted else '0')
        llm_part = _strict_llm_extract(user_text, extracted, image_urls=image_urls)
        prev_keys = set(extracted.keys())
        pre_values = {k: extracted.get(k) for k in ["category", "subcategory"] if extracted.get(k)}
        # Detect explicit correction intent when at ready stage and user negates/overrules
        correction_signal = False
        is_ready_stage = current.get("stage") == "ready"
        if is_ready_stage:
            if re.search(r"(ìˆ˜ì •|ì •ì •|ì•„ë‹ˆë¼|ì•„ë‹Œ|ë‹¤ì‹œ|í‹€ë ¸|ë°”ê¿”|ë³€ê²½|ë§(?!.*ì•Š)|ì˜¤íƒ€)", user_text):
                correction_signal = True
        # Implicit correction heuristic: short message providing a single differing field value
        implicit = False
        if is_ready_stage and not correction_signal and llm_part:
            core_fields = ["category", "subcategory", "lost_date", "region"]
            diffs = [f for f in core_fields if f in llm_part and f in extracted and llm_part[f] != extracted[f]]
            # message length & token heuristics: very short or label-prefixed
            text_len = len(user_text.strip())
            token_count = len(re.split(r"\s+", user_text.strip())) if user_text.strip() else 0
            label_prefix = bool(re.match(r"^(ì¥ì†Œ|ì§€ì—­|ë‚ ì§œ|ì¢…ë¥˜|ì¹´í…Œê³ ë¦¬|subcategory)[:\s]", user_text.strip()))
            # avoid verbs indicating new narrative (ìƒ, ì°¾, ë¶„ì‹¤ ë“±) -> more likely new item
            verb_like = re.search(r"(ìƒ|ì°¾|ë¶„ì‹¤|ë„ì™€|ê²€ìƒ‰)", user_text)
            if diffs and len(diffs) <= 2 and text_len <= 25 and token_count <= 5 and not verb_like:
                implicit = True
            elif diffs and label_prefix and not verb_like:
                implicit = True
        if implicit:
            correction_signal = True
        # Merge (allow override only for core fields during correction)
        _merge_extracted(
            extracted,
            llm_part,
            allow_override=correction_signal,
            override_keys=["category", "subcategory", "lost_date", "region"]
        )
        # Region post-processing: remove relative date words mistakenly captured as region, attempt recovery
        try:
            REL_DATE = {"ì–´ì œ","ì˜¤ëŠ˜","ê·¸ì €ê»˜","ê·¸ì œ","ì´í‹€ì „","ì‚¼ì¼ì „","3ì¼ì „","2ì¼ì „","ìµœê·¼","ë°©ê¸ˆ"}
            reg_val = extracted.get('region')
            if reg_val and reg_val in REL_DATE:
                extracted.pop('region', None)
            if not extracted.get('region'):
                # Attempt to recover place from user_text tokens
                import re as _re
                tokens = _re.split(r"\s+", user_text.strip())
                cand = None
                for t in tokens:
                    base = t
                    m = _re.match(r"(.+?)(?:ì—ì„œ|ì—)$", t)
                    if m:
                        base = m.group(1)
                        # Treat anything that had 'ì—ì„œ' suffix as a location candidate directly if length ok
                        if 2 <= len(base) <= 15 and base not in REL_DATE:
                            cand = base
                            break
                    if len(base) < 2 or len(base) > 15:
                        continue
                    # Heuristic: ends with typical location suffix OR in gazetteer list
                    if base.endswith(("ì—­","êµ¬","ë™","í„°ë¯¸ë„","ê³µí•­")) or base in {"ì¢…ë¡œ","ê°•ë‚¨","ì‹ ì´Œ","í™ëŒ€","ì ì‹¤","ê±´ëŒ€","ì¸ì²œê³µí•­"}:
                        cand = base
                        break
                if cand:
                    extracted['region'] = cand
        except Exception as _e:
            logger.warning('region_sanitize error=%s', _e)
        # Source tagging + conflict detection
        conflicts = current.get('conflicts') or {}
        vision_conf_map = {}
        if llm_part and isinstance(llm_part, dict):
            conf_dict = llm_part.get('confidences') if isinstance(llm_part.get('confidences'), dict) else {}
            # normalize confidence numbers
            for ck, cv in conf_dict.items():
                try:
                    vision_conf_map[ck] = float(cv)
                except Exception:
                    pass
            sources = current.setdefault('sources', {})
            for k in llm_part.keys():
                if k == 'confidences':
                    continue
                if k not in prev_keys and k not in sources and k in extracted:
                    sources[k] = 'vision' if image_urls else 'llm'
            # Detect conflicts for core fields (text value retained unless override criteria)
            for field in ["category", "subcategory"]:
                if field in llm_part and field in pre_values and llm_part[field] != pre_values[field]:
                    # store conflict structure
                    if field not in conflicts:
                        conflicts[field] = {
                            'text_value': pre_values[field],
                            'vision_value': llm_part[field],
                            'vision_confidence': vision_conf_map.get(field)
                        }
            # Auto override rule (category/subcategory high confidence override)
            for field in ["category", "subcategory"]:
                if field in conflicts:
                    vc = conflicts[field].get('vision_confidence') or 0.0
                    if vc >= VISION_AUTO_OVERRIDE_THRESHOLD:
                        # perform override; keep original as alt_<field>_text
                        original = conflicts[field]['text_value']
                        extracted[field] = conflicts[field]['vision_value']
                        extracted[f'alt_{field}_text'] = original
                        sources[field] = 'vision-override'
                        conflicts[field]['auto_overridden'] = True
                    else:
                        conflicts[field]['auto_overridden'] = False
            if conflicts:
                current['conflicts'] = conflicts
            current['sources'] = sources
        # Validate (will keep candidates if present and no final date)
        extracted = li_ext._validate(extracted)  # type: ignore[attr-defined]
        current["extracted"] = extracted
        current["missing"] = li_ext.compute_missing(extracted)

    # Decide next prompt (date í›„ë³´ ë¡œì§ ì œê±°)
    extracted = current.get("extracted", {})
    missing = current.get("missing", [])
    if missing:
            # Determine next field
            order = [f for f in ["category", "subcategory", "lost_date", "region"] if f in missing]
            next_field = order[0] if order else None
            # Track ask attempts
            ask_counts = current.setdefault("ask_counts", {})
            changed = extracted.keys() != prev_extracted.keys() or any(extracted.get(k) != prev_extracted.get(k) for k in prev_extracted.keys())
            if next_field:
                if (not changed) and prev_missing == missing:
                    ask_counts[next_field] = ask_counts.get(next_field, 0) + 1
                else:
                    # reset counter when progress made or field changed
                    ask_counts[next_field] = 0
                # Guess after 2 unsuccessful asks (count >=2)
                if ask_counts.get(next_field, 0) >= 2:
                    _log_metric('guess.attempt', field=next_field)
                    guess = _guess_field(extracted, next_field)
                    if guess and guess.upper() != 'UNKNOWN':
                        extracted[next_field] = guess
                        # revalidate & recompute
                        extracted = li_ext._validate(extracted)  # type: ignore[attr-defined]
                        current["extracted"] = extracted
                        current["missing"] = li_ext.compute_missing(extracted)
                        missing = current["missing"]
                        ask_counts[next_field] = 0
                        _log_metric('guess.filled', field=next_field)
            current["stage"] = "collecting"
            base = li_ext.build_missing_field_prompt(extracted, missing)
            reply = _persona_wrap(base, 'ask')
    else:
        if current.get("stage") != "confirmed":
            current["stage"] = "ready"
            confirmation = _build_confirmation_summary(extracted)
            reply = _persona_wrap(confirmation, 'confirm')
            if not current.get("media_ids") and not current.get("asked_photo"):
                reply += "\n\nì¶”ê°€ë¡œ ì‚¬ì§„ì´ ìˆë‹¤ë©´ ì§€ê¸ˆ ìµœëŒ€ 3ì¥ê¹Œì§€ ì˜¬ë ¤ì£¼ì„¸ìš”. ì—†ìœ¼ë©´ ê³„ì† ë§ì”€í•˜ê±°ë‚˜ ê²€ìƒ‰ ì§„í–‰ ì˜ì‚¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•´ ì£¼ì„¸ìš”."
                current["asked_photo"] = True
        else:
            reply = None  # no need to say anything; allow general chat

    snapshot = _snapshot(idx, current)
    # Optional final response guard polishing (only for replies we generated here)
    if reply and ENABLE_STYLE_ENRICH:
        enriched = _style_enrich(reply, intent="item", item_snapshot=snapshot)
        if enriched:
            reply = enriched
    if reply and ENABLE_RESPONSE_GUARD:
        guarded = _guard_response(reply, intent="item", model_label="lost-item-flow.v2", item_snapshot=snapshot)
        if guarded:
            reply = guarded
    return reply, lost_state, "lost-item-flow.v2", snapshot


def _snapshot(idx: int, item: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "index": idx,
        "stage": item.get("stage"),
        "extracted": item.get("extracted"),
        "missing": item.get("missing"),
    }


def _log_metric(event: str, **fields: Any) -> None:
    ts = datetime.now().isoformat()
    compact = ' '.join(f"{k}={v}" for k, v in fields.items())
    logger.info("metric ts=%s event=%s %s", ts, event, compact)


def _build_confirmation_summary(extracted: Dict[str, Any]) -> str:
    parts = []
    cat = extracted.get('category'); sub = extracted.get('subcategory')
    if cat and sub: parts.append(f"ì¢…ë¥˜: {cat} ({sub})")
    elif cat: parts.append(f"ì¢…ë¥˜: {cat}")
    # ìƒ‰ìƒ ì¶œë ¥ ì œê±°
    ld = extracted.get('lost_date');
    if ld: parts.append(f"ë‚ ì§œ: {ld}")
    reg = extracted.get('region');
    if reg: parts.append(f"ì¥ì†Œ: {reg}")
    if not parts:
        return "ì •ë³´ ì •ë¦¬ ì¤‘ì…ë‹ˆë‹¤."
    return "í™•ì¸í•´ì£¼ì„¸ìš”:\n" + "\n".join(f"- {p}" for p in parts) + "\nìˆ˜ì •í•  ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°”ë¡œ ì ì–´ì£¼ì‹œê³ , ê´œì°®ìœ¼ë©´ ê³„ì† ì§„í–‰ ì˜ì‚¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”."


def _guess_field(extracted: Dict[str, str], field: str) -> str | None:  # pragma: no cover (LLM)
    """Use LLM to guess a plausible value for a missing field after repeated user non-response.
    Returns guessed string or None/UNKNOWN.
    """
    llm = get_llm()
    context_json = json.dumps(extracted, ensure_ascii=False)
    guidelines = {
        'category': 'ê°€ëŠ¥í•œ ëŒ€ë¶„ë¥˜ ì¤‘ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ 1ê°œ (ì „ìê¸°ê¸°/ì˜ë¥˜/ê°€ë°©/ì§€ê°‘/ì•¡ì„¸ì„œë¦¬).',
        'subcategory': 'ì´ë¯¸ categoryê°€ ìˆë‹¤ë©´ ê·¸ í•˜ìœ„ ì†Œë¶„ë¥˜ ì¤‘ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ 1ê°œ.',
    # ìƒ‰ìƒ í•­ëª© ì œê±°
        'lost_date': 'ìµœê·¼ 10ì¼ ì´ë‚´ì˜ ë‚ ì§œ ì¤‘ í•©ë¦¬ì ì¸ 1ê°œ (YYYY-MM-DD). ì§€ë‚˜ì¹˜ê²Œ ì„ì˜ ëŠë‚Œ í”¼í•˜ê¸°.',
        'region': 'í•œêµ­ ë‚´ ì¼ë°˜ì ì¸ ì§€ëª… 1ê°œ (ì˜ˆ: ê°•ë‚¨, ì‹ ì´Œ, ì„œìš¸). ë„ˆë¬´ ìƒì„¸ ì£¼ì†Œ í”¼í•¨.'
    }
    system = (
        f"ë¶„ì‹¤ë¬¼ ì¶”ë¡  ë³´ì¡°ê¸°. ì£¼ì–´ì§„ partial JSON: {context_json}\n" \
        f"ì¶”ì • ëŒ€ìƒ í•„ë“œ: {field}\n" \
        f"ê·œì¹™: 1) í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ 2) ìì‹ ì—†ìœ¼ë©´ UNKNOWN 3) ì¶”ê°€ ì„¤ëª… ê¸ˆì§€ 4) ì¶œë ¥ì€ ê°’ë§Œ.\n" \
        f"ê°€ì´ë“œ: {guidelines.get(field,'ê°’ 1ê°œ')}"
    )
    try:
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": "ì¶”ì •ê°’ë§Œ ì¶œë ¥"}
        ]).strip()
    except Exception:
        return None
    # Sanitize multi-line -> first line
    if '\n' in out:
        out = out.split('\n',1)[0].strip()
    # Basic cleanup
    out = out.strip().strip('"').strip()
    if not out:
        return None
    if len(out) > 25:
        return None
    return out


def _load_response_guard_prompt() -> str:
    fname = f"app/prompts/response_guard_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _guard_response(draft: str, intent: str, model_label: str, item_snapshot: Dict[str, Any]) -> str:
    """Run lightweight LLM pass to enforce style/safety rules. Fallback to original on failure."""
    try:
        prompt_tmpl = _load_response_guard_prompt()
        if not prompt_tmpl:
            return draft
        llm = get_llm()
        item_json = json.dumps(item_snapshot.get("extracted") or {}, ensure_ascii=False)
        system = prompt_tmpl.format(
            INTENT=intent,
            MODEL_LABEL=model_label,
            ITEM_JSON=item_json,
            REPLY_DRAFT=draft.strip(),
        )
        # Use single system style: guard prompt includes all variables; user empty to avoid leakage
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": ""}
        ]).strip()
        if out:
            # basic sanity: avoid returning raw labels or json markers
            if out.count('{') > 2 and len(out) - len(out.replace('{', '')) > 2:
                return draft
            return out
        return draft
    except Exception:
        return draft


def _style_enrich(draft: str, intent: str, item_snapshot: Dict[str, Any]) -> str:
    """Lightweight style enrichment: expand tone with empathy & clarity while preserving facts.
    Keeps bullet structure, field labels, JSON fragments. Avoids adding yes/no rigid prompts.
    """
    try:
        llm = get_llm()
        item_json = json.dumps(item_snapshot.get("extracted") or {}, ensure_ascii=False)
        if ALLOW_CUTE_PERSONA:
            # Few-shot ìŠ¤íƒ€ì¼ ì˜ˆì‹œëŠ” ê°„ê²°í•˜ê²Œ í¬í•¨ (ê³¼í•œ í† í° ë‚­ë¹„ ë°©ì§€)
            examples = (
                "ì›ë¬¸: 'ì •ë³´ ì •ë¦¬ ì¤‘ì…ë‹ˆë‹¤.'\n"
                "ê°œì„ : 'ì‚´ì§ ì½”ë¥¼ í‚í‚í•˜ë©´ì„œ ì •ë¦¬ ì¤‘ì´ì—ìš”â€¦ ì ê¹ë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” ğŸ¾'\n\n"
                # ìƒ‰ìƒ ê´€ë ¨ ì˜ˆì‹œ ì œê±° (ìƒ‰ìƒ ì¶”ì¶œ ë¹„í™œì„±í™”)
            ) if CUTE_TONE_LEVEL >= 2 else ""
            intensity = {
                1: "ì€ì€í•˜ê³  ì ˆì œëœ",
                2: "ëˆˆì— ë„ì§€ë§Œ ê³¼í•˜ì§€ ì•Šì€",
                3: "ì¡°ê¸ˆ ë” ì ê·¹ì ì´ì§€ë§Œ ì—¬ì „íˆ ìì—°ìŠ¤ëŸ¬ìš´"
            }.get(CUTE_TONE_LEVEL, "ëˆˆì— ë„ì§€ë§Œ ê³¼í•˜ì§€ ì•Šì€")
            max_emoji = {1:1,2:3,3:4}.get(CUTE_TONE_LEVEL,3)
            tail_expr = {1:1,2:2,3:3}.get(CUTE_TONE_LEVEL,2)
            system = (
                f"ë„ˆëŠ” ë¶„ì‹¤ë¬¼ ë„ìš°ë¯¸ 'ê°•ì•„ì§€' ìºë¦­í„°ë‹¤. DRAFTë¥¼ ë” ë”°ëœ»í•˜ê³  {intensity} ê°•ì•„ì§€ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ì¬ì‘ì„±í•˜ë˜ ì‚¬ì‹¤/í•„ë“œ/ìˆ«ì/ë‚ ì§œëŠ” ì ˆëŒ€ ë³€í˜•í•˜ì§€ ë§ˆ. "
                "ê·œì¹™:\n"
                f"1) ë§íˆ¬: ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§+ì‚´ì§ ê·€ì—¬ìš´ ì–´ë¯¸(~ìš”, ~í–ˆì–´ìš”, ~í• ê²Œìš”). ê³¼í•œ ì• êµ ê¸ˆì§€.\n"
                f"2) ê°•ì•„ì§€ í‘œí˜„(ì˜ˆ: ê¼¬ë¦¬ ì‚´ë‘, ì½” í‚í‚, ì—´ì‹¬íˆ ì°¾ì•„ë³¼ê²Œìš”) ìµœëŒ€ {tail_expr}íšŒ.\n"
                f"3) ì´ëª¨ì§€ 0~{max_emoji}ê°œ (í—ˆìš©: ğŸ¶ ğŸ¾ ğŸ” âœ¨). ì—°ì† ì‚¬ìš© ê¸ˆì§€.\n"
                "4) í•„ìš”í•˜ë©´ í•œ ì¤„ ê³µê° (ê±±ì •ë˜ì…¨ì£  / ì˜ ì°¾ì•„ë³¼ê²Œìš” ë“±).\n"
                "5) ë¶ˆë¦¿/êµ¬ì¡°/í•µì‹¬ í‚¤ì›Œë“œ ìœ ì§€, ë¶ˆë¦¿ ì•ˆ ê°’ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê¸°.\n"
                "6) ìƒˆë¡œìš´ 'ì˜ˆ/ì•„ë‹ˆì˜¤' ê°•ìš” ë¬¸êµ¬ ë§Œë“¤ì§€ ë§ê¸°.\n"
                "7) ì¶”ì¸¡/í—ˆìœ„/ê³¼ì¥ ì¶”ê°€ ê¸ˆì§€. ëª¨ë¥´ëŠ” ê±´ ê±´ë„ˆëœ€.\n"
                "8) 2~6ë¬¸ì¥, 380ì ì´ë‚´.\n"
                "9) ì‚¬ì§„ ìš”ì²­ ì´ë¯¸ ìˆìœ¼ë©´ ë°˜ë³µí•˜ì§€ ë§ê¸°.\n"
                "10) ë™ì¼ ì´ëª¨ì§€ ë°˜ë³µ ì‚¬ìš© ìì œ.\n"
                "ì¶œë ¥: ì¬ì‘ì„± í…ìŠ¤íŠ¸ë§Œ.\n\n"
                f"ê°„ë‹¨ ì˜ˆì‹œ:\n{examples}" 
            )
        else:
            system = (
                "ë„ˆëŠ” ë‹µë³€ ìŠ¤íƒ€ì¼ í–¥ìƒ ëª¨ë“ˆ. ì…ë ¥ ì´ˆì•ˆ(DRAFT)ì„ í•œêµ­ì–´ë¡œ ë” ìì—°ìŠ¤ëŸ½ê³  ê³µê° ìˆê²Œ ë‹¤ë“¬ë˜, ì˜ë¯¸/ì‚¬ì‹¤/í•„ë“œëª…/ë‚ ì§œ/ì¹´í…Œê³ ë¦¬ ë“± í•µì‹¬ ì •ë³´ëŠ” ì ˆëŒ€ ì‚­ì œ/ì™œê³¡í•˜ì§€ ë§ˆ. "
                "ê·œì¹™:\n"
                "1) ëª©ë¡/ë¶ˆë¦¿/ì½”ë“œ/JSON êµ¬ì¡° ìœ ì§€.\n"
                "2) ê³¼í•œ ì´ëª¨ì§€ í”¼í•˜ê³  0~3ê°œ ì´ëª¨ì§€ í—ˆìš©.\n"
                "3) ì‚¬ìš©ìê°€ í¸ì•ˆíˆ ëŠë¼ë„ë¡ ì§§ì€ ê³µê° 1ë¬¸ì¥ ê°€ëŠ¥.\n"
                "4) 'ì˜ˆ/ì•„ë‹ˆì˜¤' ê°™ì€ ì´ë¶„ë²• ê°•ìš” í‘œí˜„ ì¶”ê°€í•˜ì§€ ë§ ê²ƒ.\n"
                "5) 2~5ë¬¸ì¥ ë˜ëŠ” 250ì ì´ë‚´.\n"
                "6) DRAFT ë‚´ ë¶ˆë¦¿ì€ ê·¸ëŒ€ë¡œ ë‘ê³  í•„ìš”í•˜ë©´ ë¶ˆë¦¿ ìœ„/ì•„ë˜ì— ê°„ê²° ë¬¸ì¥ë§Œ ì¶”ê°€.\n"
                "7) ì¶”ì¸¡ ì¶”ê°€ ê¸ˆì§€.\n"
                "8) ì‚¬ì§„ ìš”ì²­ ë¬¸êµ¬ê°€ ì´ë¯¸ ìˆë‹¤ë©´ ì¤‘ë³µ ìš”ì²­ í”¼í•¨.\n"
                "ì¶œë ¥ì€ ë‹¤ë“¬ì–´ì§„ í…ìŠ¤íŠ¸ë§Œ."
            )
        user = f"ITEM_JSON: {item_json}\nINTENT: {intent}\nDRAFT:\n{draft}\n\nê°œì„ ëœ ì‘ë‹µ:"  # context bundling
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]).strip()
        if not out:
            return draft
        # Basic safety: ensure core field tokens not lost (if they existed)
        core_tokens = []
        for k, v in (item_snapshot.get("extracted") or {}).items():
            if isinstance(v, str) and v and v in draft:
                core_tokens.append(v)
        missing_core = [t for t in core_tokens if t not in out]
        if missing_core:
            return draft  # revert to be safe
        if len(out) > 800:
            return draft
        return out
    except Exception:
        return draft
