"""Extraction controller orchestrating friendly persona vs structured JSON extraction.

Responsibilities:
- Decide whether to run structured extraction LLM (JSON only) based on new user input & state.
- Run fast rule extraction first, then (conditionally) strict JSON LLM extraction and merge.
- Validate & normalize fields (delegates to lost_item_extractor._validate where possible).
- Determine next conversational action (ask missing field, disambiguate, confirm, or pass-through).
- Maintain per-item stages: collecting -> ready -> confirmed.
"""
from __future__ import annotations
from typing import Dict, Any, Tuple, List, Optional
import re
import json
from datetime import datetime, timedelta
from pathlib import Path

from app.services import lost_item_extractor as li_ext
from app.domain import lost_item_schema as schema
from app.services.llm_providers import get_llm
from app.services import external_search
from config import settings

DATE_ISO_RE = re.compile(r"20\d{2}-\d{2}-\d{2}$")

PERSONA_PREFIXES: List[str] = []  # deprecated; kept for backward compatibility

# Intent keyword heuristics
SHORT_GREETING_TOKENS = ["ÏïàÎÖï", "ÏïàÎÖïÌïòÏÑ∏Ïöî", "hi", "hello", "„Öé„Öá"]
# Identity / capability inquiry patterns considered friendly opening, not off-topic
INTRO_PATTERNS = [
    r"ÎÑà.*ÎàÑÍµ¨", r"ÎÑà.*Î≠êÌïòÎäî", r"ÎÑå ÎàÑÍµ¨", r"ÎàÑÍµ¨ÏÑ∏Ïöî", r"Ï†ïÏ≤¥ÏÑ±", r"Ïó≠Ìï†Ïù¥ Î≠ê", r"Î¨¥Ïä® Ïó≠Ìï†", r"Î¨¥ÏóáÏùÑ Ìï† Ïàò", r"Î≠ê Ìï† Ïàò", r"Î¨¥Ïä® ÏùºÏùÑ", r"Ìï† Ïàò ÏûàÏñ¥\??"
]
INTENT_LABELS = ["greeting", "policy", "item", "confirm", "cancel", "other"]
_INTENT_CACHE: Dict[str, str] = {}
_INTENT_CACHE_MAX = 500

# Toggle for final response guard LLM (lightweight style/safety polish)
ENABLE_RESPONSE_GUARD = True
# Optional stylistic enrichment (makes baseline functional message warmer & empathetic)
ENABLE_STYLE_ENRICH = True
# Allow light cute persona tone (dog helper) in enrichment
ALLOW_CUTE_PERSONA = True
# 1=ÏùÄÏùÄ, 2=ÌëúÏ§Ä Í∑ÄÏó¨ÏõÄ, 3=Í∞ïÏ°∞ (Í≥ºÏûâ Î∞©ÏßÄ Î°úÏßÅ Ïú†ÏßÄ)
CUTE_TONE_LEVEL = 2
# PROMPT_VERSION now driven by environment via config.Settings (PROMPT_VERSION env var)
PROMPT_VERSION = getattr(settings, 'PROMPT_VERSION', 1)

# Vision confidence threshold for optional fields (brand/material/pattern ...)
VISION_OPTIONAL_CONF_THRESHOLD = 0.55
VISION_AUTO_OVERRIDE_THRESHOLD = 0.85  # high-confidence vision override threshold for critical fields

def _persona_wrap(prompt: str, kind: str) -> str:
    # Neutral passthrough; style handled by higher-level system prompt / guard.
    return prompt


def _classify_intent_rule(text: str) -> str:
    t = text.strip().lower()
    if not t:
        return "other"
    # short pure greeting only OR identity/intro queries
    if any(t == g.lower() or (t.startswith(g.lower()) and len(t) <= len(g)+2) for g in SHORT_GREETING_TOKENS):
        return "greeting"
    for pat in INTRO_PATTERNS:
        try:
            if re.search(pat, t):
                return "greeting"
        except Exception:
            continue
    # Lightweight item heuristics: single/short tokens that look like category/subcategory/relative date/region supplement
    hangul_only = re.sub(r"[^Í∞Ä-Ìû£]", "", t)
    token_count = len(re.split(r"\s+", t))
    # relative date keywords
    rel_date = any(kw in t for kw in ["Ïñ¥Ï†ú","Í∑∏Ï†ú","Í∑∏Ï†ÄÍªò","3Ïùº Ï†Ñ","2Ïùº Ï†Ñ","ÏßÄÎÇúÏ£º","ÏùºÏ£ºÏùº Ï†Ñ"])
    # direct category/subcategory membership or typical endings (Ïπ¥Îìú, Ïó≠, ÎåÄÏó≠)
    if token_count <= 3:
        from app.domain import lost_item_schema as _schema  # local import to avoid cycle
        if any(cat == t for cat in _schema.PRIMARY_CATEGORIES):
            return "item"
        # flatten subcategories
        if any(t == sub for subs in _schema.SUBCATEGORIES.values() for sub in subs):
            return "item"
        if rel_date:
            return "item"
        # endings suggesting region (Ïó≠) or item type (Ïπ¥Îìú, Ìè∞, ÏßÄÍ∞ë, ÎÖ∏Ìä∏Î∂Å)
        if t.endswith("Ïó≠") and len(t) >= 3:
            return "item"
        if any(t.endswith(suf) for suf in ["Ïπ¥Îìú","Ìè∞","ÏßÄÍ∞ë","ÎÖ∏Ìä∏Î∂Å"]):
            return "item"
    return "other"


def _intent_cache_key(text: str) -> str:
    return text.strip().lower()[:200]


def _evict_if_needed():
    if len(_INTENT_CACHE) > _INTENT_CACHE_MAX:
        # naive eviction: drop first 50
        for i, k in enumerate(list(_INTENT_CACHE.keys())):
            del _INTENT_CACHE[k]
            if i >= 49:
                break


def _load_intent_prompt() -> str:
    fname = f"app/prompts/intent_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _classify_intent_llm(text: str) -> str:
    llm = get_llm()
    system = _load_intent_prompt() or (
        "ÎÑàÎäî Ïù∏ÌÖêÌä∏ Î∂ÑÎ•òÍ∏∞. greeting|policy|item|confirm|cancel|other Ï§ë ÌïòÎÇòÎßå Ï∂úÎ†•."
    )
    user = f"Î©îÏãúÏßÄ: {text.strip()}\nÏûò ÏÉùÍ∞ÅÌïòÍ≥† Í∞ÄÏû• ÏïåÎßûÏùÄ ÌïòÎÇòÏùò Î†àÏù¥Î∏îÎßå Ï∂úÎ†•."  # thinking nudge
    try:
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]).strip().lower()
    except Exception:
        return "other"
    token = re.split(r"[^a-zÍ∞Ä-Ìû£]+", out)[0] if out else "other"
    if token not in INTENT_LABELS:
        # simple containment fallback
        for lbl in INTENT_LABELS:
            if lbl in out:
                token = lbl
                break
    if token not in INTENT_LABELS:
        token = "other"
    return token


def classify_intent(text: str) -> Tuple[str, str]:
    """Return (intent, source) using rule fast-path + LLM fallback + cache."""
    key = _intent_cache_key(text)
    if key in _INTENT_CACHE:
        return _INTENT_CACHE[key], "cache"
    # confirm/cancel tokens quick
    low = key
    # Removed hard token shortcut for confirm/cancel to let LLM handle nuanced affirmation/negation.
    rule = _classify_intent_rule(text)
    if rule in {"greeting"}:
        _INTENT_CACHE[key] = rule; return rule, "rule"
    # LLM fallback for item vs other nuance
    llm_intent = _classify_intent_llm(text)
    _INTENT_CACHE[key] = llm_intent
    _evict_if_needed()
    return llm_intent, "llm"


def _rule_extract(text: str) -> Dict[str, str]:
    # Directly use internal rule analyzer (safe for controlled import)
    try:
        data = li_ext._analyze_rule(text)  # type: ignore[attr-defined]
    except Exception:
        data = {}
    return data


def _load_extraction_prompt() -> str:
    fname = f"app/prompts/extraction_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""

def _load_multimodal_prompt() -> str:
    # Prefer newest v4 taxonomy-aware prompt; fallback to v3.
    for fname in ["app/prompts/multimodal_extraction_v4.txt", "app/prompts/multimodal_extraction_v3.txt"]:
        p = Path(fname)
        if p.exists():
            try:
                return p.read_text(encoding="utf-8")
            except Exception:
                continue
    return ""


def _strict_llm_extract(user_text: str, current: Dict[str, str], image_urls: Optional[List[str]] = None) -> Dict[str, Any]:  # pragma: no cover (LLM)
    """LLM JSON-only structured extraction using external prompt file with retry & light repair."""
    llm = get_llm()
    today = datetime.now().date().isoformat()
    # dynamic few-shot date replacements
    try:
        yesterday = (datetime.now().date() - timedelta(days=1)).isoformat()
        d3 = (datetime.now().date() - timedelta(days=3)).isoformat()
    except Exception:
        yesterday = today
        d3 = today
    # Choose prompt: multimodal if images present else text-only
    if image_urls:
        prompt_tmpl = _load_multimodal_prompt() or _load_extraction_prompt()
    else:
        prompt_tmpl = _load_extraction_prompt()
    current_json = json.dumps(current, ensure_ascii=False) if current else "{}"
    if prompt_tmpl:
        placeholders = {
            'TODAY': today,
            'YESTERDAY': yesterday,
            'D3': d3,
            'D2': (datetime.now().date() - timedelta(days=2)).isoformat(),
            'D7': (datetime.now().date() - timedelta(days=7)).isoformat(),
            'CATEGORIES': ", ".join(schema.PRIMARY_CATEGORIES),
            'CURRENT_JSON': current_json,
            'USER_TEXT': user_text.strip(),
            'IMAGE_COUNT': str(len(image_urls) if image_urls else 0),
        }
        system = prompt_tmpl
        for k, v in placeholders.items():
            system = system.replace('{'+k+'}', v)
    else:
        system = "Îã®Ïùº JSON {category, subcategory, lost_date, region}; ÏóÜÎäî Í∞í ÌÇ§ ÏÉùÎûµ; today=" + today
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": system}
    ]
    # Vision ÏßÄÏõê (OpenAI 4o-mini Îì±) - provider Í∞Ä openai Ïù¥Í≥† Ïù¥ÎØ∏ÏßÄ URL Ïù¥ ÏûàÏúºÎ©¥ Î©ÄÌã∞Î™®Îã¨ parts Íµ¨ÏÑ±
    llm_name = getattr(llm, 'name', '')
    if image_urls and llm_name == 'openai':
        parts: List[Dict[str, Any]] = []
        if user_text.strip():
            parts.append({"type": "text", "text": user_text.strip()})
        for url in image_urls[:3]:  # safety cap
            parts.append({"type": "image_url", "image_url": {"url": url}})
        # Ïù¥ÎØ∏ÏßÄÎßå ÏûàÎäî Í≤ΩÏö∞Î•º ÏúÑÌï¥ ÏµúÏÜå ÌïúÍ∞úÏùò text guidance Ï∂îÍ∞Ä (Ï∂îÏ∂ú ÏßÄÏãú)
        if not user_text.strip():
            parts.insert(0, {"type": "text", "text": "Ïù¥ÎØ∏ÏßÄÏóê Î≥¥Ïù¥Îäî Î∂ÑÏã§Î¨º Ï†ïÎ≥¥Î•º category, subcategory, lost_date(Î™®Î•¥Î©¥ ÎπàÏπ∏), region(Î™®Î•¥Î©¥ ÎπàÏπ∏) JSON Ï∂îÏ∂ú"})
        messages.append({"role": "user", "content": parts})
    else:
        messages.append({"role": "user", "content": user_text.strip()})

    def _attempt_parse(raw: str) -> Dict[str, str] | None:
        snippet = raw.strip()
        if '```' in snippet:
            parts = snippet.split('```')
            # choose middle part if possible
            for p in parts:
                if '{' in p and '}' in p:
                    snippet = p; break
        snippet = snippet.strip()
        snippet = re.sub(r'^json\n', '', snippet, flags=re.IGNORECASE).strip()
        # direct parse
        try:
            parsed = json.loads(snippet)
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            pass
        # salvage braces
        m = re.search(r"\{.*?\}", raw, flags=re.DOTALL)
        if m:
            try:
                parsed = json.loads(m.group(0))
                if isinstance(parsed, dict):
                    return parsed
            except Exception:
                pass
        return None

    def _clean(parsed: Dict[str, Any]) -> Dict[str, Any]:
        cleaned: Dict[str, Any] = {}
        scalar_keys = ["category", "subcategory", "lost_date", "region", "brand", "material", "pattern"]
        for k in scalar_keys:
            v = parsed.get(k)
            if isinstance(v, str):
                t = v.strip()
                if t:
                    cleaned[k] = t[:50]
        # Arrays
        def _norm_list(vals, limit):
            out = []
            if isinstance(vals, list):
                for x in vals:
                    if not isinstance(x, str):
                        continue
                    s = x.strip()
                    if not s:
                        continue
                    if len(s) > 25:
                        s = s[:25]
                    if s not in out:
                        out.append(s)
                    if len(out) >= limit:
                        break
            return out
        nf = _norm_list(parsed.get('notable_features'), 5)
        if nf:
            cleaned['notable_features'] = nf
        ts = _norm_list(parsed.get('text_snippets'), 4)
        # rudimentary PII filter: remove email-like / long digit sequences
        filtered_ts = []
        for s in ts:
            if '@' in s:
                continue
            if sum(c.isdigit() for c in s) >= 6:
                continue
            filtered_ts.append(s)
        if filtered_ts:
            cleaned['text_snippets'] = filtered_ts
        # confidences
        confidences = parsed.get('confidences')
        kept_conf: Dict[str, float] = {}
        if isinstance(confidences, dict):
            for k, v in confidences.items():
                try:
                    f = float(v)
                except Exception:
                    continue
                if f < 0 or f > 1:
                    continue
                kept_conf[k] = round(f, 2)
        # Apply threshold removal for optional fields
        for opt_key in ["brand", "material", "pattern"]:
            if opt_key in cleaned:
                conf_v = kept_conf.get(opt_key)
                if conf_v is not None and conf_v < VISION_OPTIONAL_CONF_THRESHOLD:
                    # drop field & its confidence entry
                    cleaned.pop(opt_key, None)
                    kept_conf.pop(opt_key, None)
        if kept_conf:
            cleaned['confidences'] = kept_conf
        return cleaned

    raw = ""
    for attempt in range(2):  # one retry if first malformed
        try:
            raw = llm.generate(messages).strip()
        except Exception:
            return {}
        parsed = _attempt_parse(raw)
        if parsed is not None:
            return _clean(parsed)
        # repair hint for retry: force user to output ONLY JSON
        messages.append({"role": "user", "content": "JSON ÌòïÏãù Ïò§Î•ò. ÏàúÏàò JSON Í∞ùÏ≤¥Îßå Îã§Ïãú Ï∂úÎ†•."})
    return {}


def _merge_extracted(base: Dict[str, str], new: Dict[str, str], *, allow_override: bool = False, override_keys: Optional[List[str]] = None) -> Dict[str, str]:
    """Merge newly extracted fields into base.

    Default: non-destructive (first value wins) to avoid oscillation.
    If allow_override=True (e.g. explicit user correction at confirmation stage),
    then keys in override_keys (or all keys if None) can be overwritten when new has a non-empty differing value.
    """
    if not new:
        return base
    if allow_override:
        if override_keys is None:
            override_keys = list(new.keys())
        for k, v in new.items():
            if not isinstance(v, str):
                continue
            if k in override_keys and v and v.strip():
                # overwrite only if different
                if base.get(k) != v:
                    base[k] = v
        # Ensure we still add missing keys (if override_keys limited)
        for k, v in new.items():
            if k not in base and isinstance(v, str):
                base[k] = v
    else:
        for k, v in new.items():
            if k not in base and isinstance(v, str):
                base[k] = v
    return base


# _has_ambiguity: removed (was unused after logic refactor)


CONFIRM_TOKENS: set[str] = set()  # deprecated
CANCEL_TOKENS: set[str] = set()
OFF_TOPIC_KEYWORDS = [
    "ÎÇ†Ïî®", "Ï£ºÏãù", "Ìà¨Ïûê", "ÏΩîÏù∏", "ÏïîÌò∏ÌôîÌèê", "Í±¥Í∞ï", "Î≥ëÏõê", "ÏßÑÎ£å", "ÏãùÎã®", "ÏïÑÏπ®Ïóê Î≠ê", "ÏöîÎ¶¨", "Î†àÏãúÌîº",
    "Ïö¥Îèô", "Îã§Ïù¥Ïñ¥Ìä∏", "Í≥µÎ∂Ä", "ÏãúÌóò", "Ïó¨Ìñâ", "Ìò∏ÌÖî"
]
SOFT_LOCK_THRESHOLD = 3


def should_run_extraction(item: Dict[str, Any], user_text: str) -> bool:
    """Always run extraction in collecting/ready unless the message is a pure confirm/cancel token.
    This favors accuracy over minimal LLM calls.
    """
    stage = item.get("stage")
    if stage not in {"collecting", "ready"}:
        return False
    # Always run extraction; confirmation now inferred at higher layer (LLM intent), not by raw tokens.
    _log_metric('extraction.trigger', reason='always')
    return True


def process_message(user_text: str, lost_state: Dict[str, Any], start_new: bool, image_urls: Optional[List[str]] = None) -> Tuple[str | None, Dict[str, Any], str, Dict[str, Any] | None]:
    """Process user message relative to lost item flow.
    Returns (assistant_reply_or_none, updated_lost_state, model_label, active_item_snapshot).
    If assistant_reply_or_none is None the caller may proceed with general LLM chat.
    """
    if lost_state is None:
        lost_state = {"items": [], "active_index": None}

    if start_new or not lost_state.get("items"):
        lost_state["items"].append({"extracted": {}, "missing": li_ext.compute_missing({}), "stage": "collecting"})
        lost_state["active_index"] = len(lost_state["items"]) - 1

    idx = lost_state.get("active_index")
    if idx is None:
        return None, lost_state, "lost-item-flow.v2", None
    current = lost_state["items"][idx]
    user_lower = user_text.strip().lower()

    # Intent detection BEFORE extraction (greeting/policy may bypass item flow)
    intent, intent_source = classify_intent(user_text)
    _log_metric('intent.classified', intent=intent, source=intent_source)
    # Soft-lock handling: if previously locked and user still off-topic, short circuit
    if lost_state.get("soft_lock") and intent not in {"item", "greeting"}:
        reply = _persona_wrap(
            "ÏÑúÎπÑÏä§Í∞Ä Î∂ÑÏã§Î¨º ÌöåÏàò ÏßÄÏõê Î™®ÎìúÎ°ú Ïû†Ïãú Ïû†Í≤ºÏñ¥Ïöî. Î∂ÑÏã§Ìïú Î¨ºÍ±¥Ïùò Ï¢ÖÎ•ò, Ïû•ÏÜå, ÎÇ†Ïßú Ï§ë ÌïòÎÇòÎùºÎèÑ ÏïåÎ†§Ï£ºÏãúÎ©¥ Îã§Ïãú ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî.",
            'ask'
        )
        return reply, lost_state, "intent:locked", _snapshot(idx, current)
    # Allow greeting to release soft lock naturally
    if lost_state.get("soft_lock") and intent == 'greeting':
        lost_state.pop("soft_lock", None)

    if intent in {"greeting", "policy", "other"}:
        if intent == "greeting":
            # Pass through: allow general LLM (system prompt) to craft intro instead of fixed template
            return None, lost_state, "intent:greeting-pass", _snapshot(idx, current)
        if intent == "policy":
            reply = (
                "Î∂ÑÏã§Î¨ºÏùÄ Î∞úÍ≤¨ ÌõÑ Í≤ΩÏ∞∞/Ïú†Ïã§Î¨º ÏÑºÌÑ∞Ïóê Ïù∏Í≥ÑÎêòÎ©¥ ÌÜµÏÉÅ ÏùºÏ†ï Í∏∞Í∞Ñ(Ïòà: 6Í∞úÏõî Ï†ÑÌõÑ, ÌíàÎ™©/Î≤ïÍ∑úÏóê Îî∞Îùº Ï∞®Ïù¥) Î≥¥Í¥Ä ÌõÑ Ï≤òÎ¶¨Îê©ÎãàÎã§. "
                "Ï†ïÌôïÌïú ÏµúÏã† Î≥¥Í¥Ä Í∏∞Í∞ÑÏùÄ Í¥ÄÌï† Í≤ΩÏ∞∞Ï≤≠/ÏßÄÏûêÏ≤¥ Í≥µÏßÄ ÎòêÎäî Í≥µÏãù Ïú†Ïã§Î¨º ÌÜµÌï©Ìè¨ÌÑ∏ÏùÑ ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî. \n"
                "Î¨ºÍ±¥ Ï†ïÎ≥¥Î•º ÏïåÎ†§Ï£ºÏãúÎ©¥ Î∞îÎ°ú Íµ¨Ï°∞ÌôîÌï¥ÏÑú Í≤ÄÏÉâ Ï§ÄÎπÑÎ•º ÎèÑÏôÄÎìúÎ¶¥Í≤åÏöî."
            )
            return _persona_wrap(reply, 'ask'), lost_state, "intent:policy", _snapshot(idx, current)
        # other (off-topic): block general chat & gently/firmly redirect
        lowered = user_text.lower()
        keyword_hit = next((kw for kw in OFF_TOPIC_KEYWORDS if kw in lowered), None)
        count = lost_state.get("off_topic_count", 0) + 1
        lost_state["off_topic_count"] = count
        if count > SOFT_LOCK_THRESHOLD:
            lost_state["soft_lock"] = True
            _log_metric('intent.soft_lock', count=count)
            msg = "Î∂ÑÏã§Î¨º Ï†ïÎ≥¥ ÏóÜÏù¥ Îã§Î•∏ Ï£ºÏ†ú ÎåÄÌôîÍ∞Ä Í≥ÑÏÜçÎêòÏñ¥ Ïû†Ïãú Ïû†Í∏à ÏÉÅÌÉúÏûÖÎãàÎã§. Ïòà: 'Ïñ¥Ï†ú Í∞ïÎÇ®ÏóêÏÑú Í≤ÄÏ†ï Î∞±Ìå© ÏûÉÏñ¥Î≤ÑÎ†∏Ïñ¥Ïöî' Ï≤òÎüº ÏïåÎ†§Ï£ºÏãúÎ©¥ Îã§Ïãú ÎèÑÏôÄÎìúÎ¶¥Í≤åÏöî."
            return _persona_wrap(msg, 'ask'), lost_state, "intent:redirect-lock", _snapshot(idx, current)
        if keyword_hit:
            msg = f"Ìï¥Îãπ Ï£ºÏ†ú(Ïòà: {keyword_hit})Îäî Ïù¥ ÏÑúÎπÑÏä§ Î≤îÏúÑÎ•º Î≤óÏñ¥ÎÇòÏöî. Î∂ÑÏã§Ìïú Î¨ºÍ±¥Ïùò Ï¢ÖÎ•òÏôÄ Ïû•ÏÜå, ÎÇ†Ïßú Ï§ë ÌïòÎÇòÎùºÎèÑ ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎèÑÏôÄÎìúÎ¶¥Í≤åÏöî. Ïòà: '3Ïùº Ï†Ñ Ïã†Ï¥åÏóêÏÑú ÌååÎûÄ Ìú¥ÎåÄÌè∞ ÏûÉÏñ¥Î≤ÑÎ†∏Ïñ¥Ïöî'"
        elif count == 1:
            msg = (
                "Ïù¥ ÏÑúÎπÑÏä§Îäî Î∂ÑÏã§Î¨º ÌöåÏàò ÏßÄÏõêÏóê ÏßëÏ§ëÌï¥Ïöî. Ïñ¥Îñ§ Î¨ºÍ±¥ÏùÑ Ïñ¥ÎîîÏÑú Ïñ∏Ï†úÏØ§ ÏûÉÏñ¥Î≤ÑÎ†∏ÎäîÏßÄ ÏïåÎ†§Ï£ºÏãúÎ©¥ ÎèÑÏôÄÎìúÎ¶¥Í≤åÏöî. "
                "Ïòà: 'Ïñ¥Ï†ú Í∞ïÎÇ®ÏóêÏÑú ÌååÎûÄ Ìú¥ÎåÄÌè∞ ÏûÉÏñ¥Î≤ÑÎ†∏Ïñ¥Ïöî'"
            )
        elif count == 2:
            msg = (
                "Î®ºÏ†Ä Î∂ÑÏã§Î¨º Í∏∞Î≥∏ Ï†ïÎ≥¥(Ïπ¥ÌÖåÍ≥†Î¶¨/ÎÇ†Ïßú/Ïû•ÏÜå)Í∞Ä ÌïÑÏöîÌï¥Ïöî. Ïòà: 'ÏßÄÎÇúÏ£º Ïã†Ï¥åÏóêÏÑú Î∞±Ìå© ÏûÉÏñ¥Î≤ÑÎ†∏Ïñ¥Ïöî' Ï≤òÎüº ÏïåÎ†§Ï£ºÏÑ∏Ïöî." )
        else:
            msg = "Î∂ÑÏã§Î¨º Ï†ïÎ≥¥(Ïòà: Ïñ∏Ï†ú, Ïñ¥ÎîîÏÑú, Ïñ¥Îñ§ Î¨ºÍ±¥)Î•º ÏûÖÎ†•Ìï¥Ï£ºÏÖîÏïº Í≥ÑÏÜç ÎèÑÏôÄÎìúÎ¶¥ Ïàò ÏûàÏñ¥Ïöî."
        return _persona_wrap(msg, 'ask'), lost_state, "intent:redirect", _snapshot(idx, current)

    # confirm/cancel intents outside ready stage ‚Üí treat politely as guidance
    if intent == 'confirm' and current.get('stage') != 'ready':
        # Provide guidance on missing fields
        missing = current.get('missing') or li_ext.compute_missing(current.get('extracted', {}))
        if missing:
            base = li_ext.build_missing_field_prompt(current.get('extracted', {}), missing)
            return _persona_wrap("ÏïÑÏßÅ ÌôïÏù∏ Ï†ÑÏù¥ÏóêÏöî. " + base, 'ask'), lost_state, 'intent:confirm-misplaced', _snapshot(idx, current)
    if intent == 'cancel' and current.get('stage') != 'ready':
        # Just encourage providing info
        return _persona_wrap("ÏßÑÌñâÏùÑ Ï∑®ÏÜåÌï† Îã®Í≥ÑÍ∞Ä ÏïÑÏßÅ ÏïÑÎãàÏóêÏöî. Î¨ºÍ±¥ Ï†ïÎ≥¥Î•º Ï°∞Í∏à Îçî ÏïåÎ†§Ï£ºÏã§ÎûòÏöî?", 'ask'), lost_state, 'intent:cancel-misplaced', _snapshot(idx, current)

    # Confirmation handling (only when intent=item)
    if current.get("stage") == "ready" and intent == 'confirm':
        current["stage"] = "confirmed"
        snapshot = _snapshot(idx, current)
        # Run approximate external search now (after explicit user confirm)
        search_msg = "Í≥µÍ∞ú ÏäµÎìùÎ¨º Í∑ºÏÇ¨ Í≤ÄÏÉâÏùÑ ÏãúÏûëÌï©ÎãàÎã§."
        try:
            matches = external_search.approximate_external_matches(current.get("extracted") or {}, max_results=5)
            if matches:
                current['external_matches'] = matches
                summary_line = external_search.summarize_matches(matches, limit=3)
                search_msg += "\n\n" + summary_line + "\nÎçî Î≥¥Í≥† Ïã∂ÏúºÏãúÎ©¥ 'ÌõÑÎ≥¥ ÏûêÏÑ∏Ìûà'ÎùºÍ≥† ÎßêÏîÄÌï¥Ï£ºÏÑ∏Ïöî."
            else:
                search_msg += "\n(ÌòÑÏû¨ Ï†ïÎ≥¥Î°ú Ï¶âÏãú Ïú†ÏÇ¨ ÌõÑÎ≥¥ ÏóÜÏùå)"
        except Exception as e:
            print('[external_search] confirm-stage search failed', e)
            search_msg += "\n(Í≤ÄÏÉâ Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏñ¥Ïöî. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.)"
        search_msg += "\nÎã§Î•∏ Î∂ÑÏã§Î¨ºÏùÑ ÏÉàÎ°ú Îì±Î°ùÌïòÎ†§Î©¥ 'ÏÉà Î¨ºÍ±¥'Ïù¥ÎùºÍ≥† ÏûÖÎ†•ÌïòÏãúÎ©¥ Îê©ÎãàÎã§."
        snapshot = _snapshot(idx, current)
        return (search_msg, lost_state, "lost-item-flow.v2", snapshot)
    if current.get("stage") == "ready" and intent == 'cancel':
        current["stage"] = "collecting"
        # fall through to ask again

    # If confirmed and user gives more info without start_new trigger -> start a new item
    if current.get("stage") == "confirmed" and not start_new:
        lost_state["items"].append({"extracted": {}, "missing": li_ext.compute_missing({}), "stage": "collecting"})
        lost_state["active_index"] = len(lost_state["items"]) - 1
        idx = lost_state["active_index"]
        current = lost_state["items"][idx]

    # If previously soft-locked and now user provides item intent, unlock
    if lost_state.get("soft_lock") and intent == 'item':
        lost_state.pop("soft_lock", None)
        _log_metric('intent.soft_lock_release')

    # Extraction decision
    prev_extracted = current.get("extracted", {}).copy()
    prev_missing = current.get("missing", [])
    if should_run_extraction(current, user_text):
        extracted = current.get("extracted", {}).copy()
        # Rule first
        rule_part = _rule_extract(user_text)
        _merge_extracted(extracted, rule_part)
        # Always call strict LLM for potential refinement (now vision-aware)
        missing_before = li_ext.compute_missing(extracted)
        _log_metric('extraction.llm_call', missing=len(missing_before), ambiguity='1' if 'lost_date_candidates' in extracted else '0')
        llm_part = _strict_llm_extract(user_text, extracted, image_urls=image_urls)
        prev_keys = set(extracted.keys())
        pre_values = {k: extracted.get(k) for k in ["category", "subcategory"] if extracted.get(k)}
        # Detect explicit correction intent when at ready stage and user negates/overrules
        correction_signal = False
        is_ready_stage = current.get("stage") == "ready"
        if is_ready_stage:
            if re.search(r"(ÏàòÏ†ï|Ï†ïÏ†ï|ÏïÑÎãàÎùº|ÏïÑÎãå|Îã§Ïãú|ÌãÄÎ†∏|Î∞îÍøî|Î≥ÄÍ≤Ω|Îßû(?!.*Ïïä)|Ïò§ÌÉÄ)", user_text):
                correction_signal = True
        # Implicit correction heuristic: short message providing a single differing field value
        implicit = False
        if is_ready_stage and not correction_signal and llm_part:
            core_fields = ["category", "subcategory", "lost_date", "region"]
            diffs = [f for f in core_fields if f in llm_part and f in extracted and llm_part[f] != extracted[f]]
            # message length & token heuristics: very short or label-prefixed
            text_len = len(user_text.strip())
            token_count = len(re.split(r"\s+", user_text.strip())) if user_text.strip() else 0
            label_prefix = bool(re.match(r"^(Ïû•ÏÜå|ÏßÄÏó≠|ÎÇ†Ïßú|Ï¢ÖÎ•ò|Ïπ¥ÌÖåÍ≥†Î¶¨|subcategory)[:\s]", user_text.strip()))
            # avoid verbs indicating new narrative (ÏûÉ, Ï∞æ, Î∂ÑÏã§ Îì±) -> more likely new item
            verb_like = re.search(r"(ÏûÉ|Ï∞æ|Î∂ÑÏã§|ÎèÑÏôÄ|Í≤ÄÏÉâ)", user_text)
            if diffs and len(diffs) <= 2 and text_len <= 25 and token_count <= 5 and not verb_like:
                implicit = True
            elif diffs and label_prefix and not verb_like:
                implicit = True
        if implicit:
            correction_signal = True
        # Merge (allow override only for core fields during correction)
        _merge_extracted(
            extracted,
            llm_part,
            allow_override=correction_signal,
            override_keys=["category", "subcategory", "lost_date", "region"]
        )
        # Region post-processing: remove relative date words mistakenly captured as region, attempt recovery
        try:
            REL_DATE = {"Ïñ¥Ï†ú","Ïò§Îäò","Í∑∏Ï†ÄÍªò","Í∑∏Ï†ú","Ïù¥ÌãÄÏ†Ñ","ÏÇºÏùºÏ†Ñ","3ÏùºÏ†Ñ","2ÏùºÏ†Ñ","ÏµúÍ∑º","Î∞©Í∏à"}
            reg_val = extracted.get('region')
            if reg_val and reg_val in REL_DATE:
                extracted.pop('region', None)
            if not extracted.get('region'):
                # Attempt to recover place from user_text tokens
                import re as _re
                tokens = _re.split(r"\s+", user_text.strip())
                cand = None
                for t in tokens:
                    base = t
                    m = _re.match(r"(.+?)(?:ÏóêÏÑú|Ïóê)$", t)
                    if m:
                        base = m.group(1)
                        # Treat anything that had 'ÏóêÏÑú' suffix as a location candidate directly if length ok
                        if 2 <= len(base) <= 15 and base not in REL_DATE:
                            cand = base
                            break
                    if len(base) < 2 or len(base) > 15:
                        continue
                    # Heuristic: ends with typical location suffix OR in gazetteer list
                    if base.endswith(("Ïó≠","Íµ¨","Îèô","ÌÑ∞ÎØ∏ÎÑê","Í≥µÌï≠")) or base in {"Ï¢ÖÎ°ú","Í∞ïÎÇ®","Ïã†Ï¥å","ÌôçÎåÄ","Ïû†Ïã§","Í±¥ÎåÄ","Ïù∏Ï≤úÍ≥µÌï≠"}:
                        cand = base
                        break
                if cand:
                    extracted['region'] = cand
        except Exception as _e:
            print('[extraction.region_sanitize] error', _e)
        # Source tagging + conflict detection
        conflicts = current.get('conflicts') or {}
        vision_conf_map = {}
        if llm_part and isinstance(llm_part, dict):
            conf_dict = llm_part.get('confidences') if isinstance(llm_part.get('confidences'), dict) else {}
            # normalize confidence numbers
            for ck, cv in conf_dict.items():
                try:
                    vision_conf_map[ck] = float(cv)
                except Exception:
                    pass
            sources = current.setdefault('sources', {})
            for k in llm_part.keys():
                if k == 'confidences':
                    continue
                if k not in prev_keys and k not in sources and k in extracted:
                    sources[k] = 'vision' if image_urls else 'llm'
            # Detect conflicts for core fields (text value retained unless override criteria)
            for field in ["category", "subcategory"]:
                if field in llm_part and field in pre_values and llm_part[field] != pre_values[field]:
                    # store conflict structure
                    if field not in conflicts:
                        conflicts[field] = {
                            'text_value': pre_values[field],
                            'vision_value': llm_part[field],
                            'vision_confidence': vision_conf_map.get(field)
                        }
            # Auto override rule (category/subcategory high confidence override)
            for field in ["category", "subcategory"]:
                if field in conflicts:
                    vc = conflicts[field].get('vision_confidence') or 0.0
                    if vc >= VISION_AUTO_OVERRIDE_THRESHOLD:
                        # perform override; keep original as alt_<field>_text
                        original = conflicts[field]['text_value']
                        extracted[field] = conflicts[field]['vision_value']
                        extracted[f'alt_{field}_text'] = original
                        sources[field] = 'vision-override'
                        conflicts[field]['auto_overridden'] = True
                    else:
                        conflicts[field]['auto_overridden'] = False
            if conflicts:
                current['conflicts'] = conflicts
            current['sources'] = sources
        # Validate (will keep candidates if present and no final date)
        extracted = li_ext._validate(extracted)  # type: ignore[attr-defined]
        current["extracted"] = extracted
        current["missing"] = li_ext.compute_missing(extracted)

    # Decide next prompt (date ÌõÑÎ≥¥ Î°úÏßÅ Ï†úÍ±∞)
    extracted = current.get("extracted", {})
    missing = current.get("missing", [])
    if missing:
            # Determine next field
            order = [f for f in ["category", "subcategory", "lost_date", "region"] if f in missing]
            next_field = order[0] if order else None
            # Track ask attempts
            ask_counts = current.setdefault("ask_counts", {})
            changed = extracted.keys() != prev_extracted.keys() or any(extracted.get(k) != prev_extracted.get(k) for k in prev_extracted.keys())
            if next_field:
                if (not changed) and prev_missing == missing:
                    ask_counts[next_field] = ask_counts.get(next_field, 0) + 1
                else:
                    # reset counter when progress made or field changed
                    ask_counts[next_field] = 0
                # Guess after 2 unsuccessful asks (count >=2)
                if ask_counts.get(next_field, 0) >= 2:
                    _log_metric('guess.attempt', field=next_field)
                    guess = _guess_field(extracted, next_field)
                    if guess and guess.upper() != 'UNKNOWN':
                        extracted[next_field] = guess
                        # revalidate & recompute
                        extracted = li_ext._validate(extracted)  # type: ignore[attr-defined]
                        current["extracted"] = extracted
                        current["missing"] = li_ext.compute_missing(extracted)
                        missing = current["missing"]
                        ask_counts[next_field] = 0
                        _log_metric('guess.filled', field=next_field)
            current["stage"] = "collecting"
            base = li_ext.build_missing_field_prompt(extracted, missing)
            reply = _persona_wrap(base, 'ask')
    else:
        if current.get("stage") != "confirmed":
            current["stage"] = "ready"
            confirmation = _build_confirmation_summary(extracted)
            reply = _persona_wrap(confirmation, 'confirm')
            if not current.get("media_ids") and not current.get("asked_photo"):
                reply += "\n\nÏ∂îÍ∞ÄÎ°ú ÏÇ¨ÏßÑÏù¥ ÏûàÎã§Î©¥ ÏßÄÍ∏à ÏµúÎåÄ 3Ïû•ÍπåÏßÄ Ïò¨Î†§Ï£ºÏÑ∏Ïöî. ÏóÜÏúºÎ©¥ Í∑∏ÎÉ• Í≥ÑÏÜç ÎßêÏîÄÌïòÍ±∞ÎÇò Í≤ÄÏÉâ ÏßÑÌñâ ÏùòÏÇ¨Î•º ÏûêÏó∞Ïä§ÎüΩÍ≤å ÌëúÌòÑÌï¥ Ï£ºÏÑ∏Ïöî."
                current["asked_photo"] = True
        else:
            reply = None  # no need to say anything; allow general chat

    snapshot = _snapshot(idx, current)
    # Optional final response guard polishing (only for replies we generated here)
    if reply and ENABLE_STYLE_ENRICH:
        enriched = _style_enrich(reply, intent="item", item_snapshot=snapshot)
        if enriched:
            reply = enriched
    if reply and ENABLE_RESPONSE_GUARD:
        guarded = _guard_response(reply, intent="item", model_label="lost-item-flow.v2", item_snapshot=snapshot)
        if guarded:
            reply = guarded
    return reply, lost_state, "lost-item-flow.v2", snapshot


def _snapshot(idx: int, item: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "index": idx,
        "stage": item.get("stage"),
        "extracted": item.get("extracted"),
        "missing": item.get("missing"),
    }


def _log_metric(event: str, **fields: Any) -> None:
    ts = datetime.now().isoformat()
    compact = ' '.join(f"{k}={v}" for k, v in fields.items())
    print(f"[metric] {ts} {event} {compact}")


def _build_confirmation_summary(extracted: Dict[str, Any]) -> str:
    parts = []
    cat = extracted.get('category'); sub = extracted.get('subcategory')
    if cat and sub: parts.append(f"Ï¢ÖÎ•ò: {cat} ({sub})")
    elif cat: parts.append(f"Ï¢ÖÎ•ò: {cat}")
    # ÏÉâÏÉÅ Ï∂úÎ†• Ï†úÍ±∞
    ld = extracted.get('lost_date');
    if ld: parts.append(f"ÎÇ†Ïßú: {ld}")
    reg = extracted.get('region');
    if reg: parts.append(f"Ïû•ÏÜå: {reg}")
    if not parts:
        return "Ï†ïÎ≥¥ Ï†ïÎ¶¨ Ï§ëÏûÖÎãàÎã§."
    return "ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî:\n" + "\n".join(f"- {p}" for p in parts) + "\nÏàòÏ†ïÌï† ÎÇ¥Ïö©Ïù¥ ÏûàÏúºÎ©¥ Î∞îÎ°ú Ï†ÅÏñ¥Ï£ºÏãúÍ≥†, Í¥úÏ∞ÆÏúºÎ©¥ Í≥ÑÏÜç ÏßÑÌñâ ÏùòÏÇ¨Î•º ÏûêÏó∞Ïä§ÎüΩÍ≤å ÌëúÌòÑÌï¥Ï£ºÏÑ∏Ïöî."


def _guess_field(extracted: Dict[str, str], field: str) -> str | None:  # pragma: no cover (LLM)
    """Use LLM to guess a plausible value for a missing field after repeated user non-response.
    Returns guessed string or None/UNKNOWN.
    """
    llm = get_llm()
    context_json = json.dumps(extracted, ensure_ascii=False)
    guidelines = {
        'category': 'Í∞ÄÎä•Ìïú ÎåÄÎ∂ÑÎ•ò Ï§ë Í∞ÄÏû• Í∞ÄÎä•ÏÑ± ÎÜíÏùÄ 1Í∞ú (Ï†ÑÏûêÍ∏∞Í∏∞/ÏùòÎ•ò/Í∞ÄÎ∞©/ÏßÄÍ∞ë/Ïï°ÏÑ∏ÏÑúÎ¶¨).',
        'subcategory': 'Ïù¥ÎØ∏ categoryÍ∞Ä ÏûàÎã§Î©¥ Í∑∏ ÌïòÏúÑ ÏÜåÎ∂ÑÎ•ò Ï§ë Í∞ÄÏû• Í∞ÄÎä•ÏÑ± ÎÜíÏùÄ 1Í∞ú.',
    # ÏÉâÏÉÅ Ìï≠Î™© Ï†úÍ±∞
        'lost_date': 'ÏµúÍ∑º 10Ïùº Ïù¥ÎÇ¥Ïùò ÎÇ†Ïßú Ï§ë Ìï©Î¶¨Ï†ÅÏù∏ 1Í∞ú (YYYY-MM-DD). ÏßÄÎÇòÏπòÍ≤å ÏûÑÏùò ÎäêÎÇå ÌîºÌïòÍ∏∞.',
        'region': 'ÌïúÍµ≠ ÎÇ¥ ÏùºÎ∞òÏ†ÅÏù∏ ÏßÄÎ™Ö 1Í∞ú (Ïòà: Í∞ïÎÇ®, Ïã†Ï¥å, ÏÑúÏö∏). ÎÑàÎ¨¥ ÏÉÅÏÑ∏ Ï£ºÏÜå ÌîºÌï®.'
    }
    system = (
        f"Î∂ÑÏã§Î¨º Ï∂îÎ°† Î≥¥Ï°∞Í∏∞. Ï£ºÏñ¥ÏßÑ partial JSON: {context_json}\n" \
        f"Ï∂îÏ†ï ÎåÄÏÉÅ ÌïÑÎìú: {field}\n" \
        f"Í∑úÏπô: 1) Ìïú Îã®Ïñ¥ ÎòêÎäî ÏßßÏùÄ Íµ¨ 2) ÏûêÏã†ÏóÜÏúºÎ©¥ UNKNOWN 3) Ï∂îÍ∞Ä ÏÑ§Î™Ö Í∏àÏßÄ 4) Ï∂úÎ†•ÏùÄ Í∞íÎßå.\n" \
        f"Í∞ÄÏù¥Îìú: {guidelines.get(field,'Í∞í 1Í∞ú')}"
    )
    try:
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": "Ï∂îÏ†ïÍ∞íÎßå Ï∂úÎ†•"}
        ]).strip()
    except Exception:
        return None
    # Sanitize multi-line -> first line
    if '\n' in out:
        out = out.split('\n',1)[0].strip()
    # Basic cleanup
    out = out.strip().strip('"').strip()
    if not out:
        return None
    if len(out) > 25:
        return None
    return out


def _load_response_guard_prompt() -> str:
    fname = f"app/prompts/response_guard_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _guard_response(draft: str, intent: str, model_label: str, item_snapshot: Dict[str, Any]) -> str:
    """Run lightweight LLM pass to enforce style/safety rules. Fallback to original on failure."""
    try:
        prompt_tmpl = _load_response_guard_prompt()
        if not prompt_tmpl:
            return draft
        llm = get_llm()
        item_json = json.dumps(item_snapshot.get("extracted") or {}, ensure_ascii=False)
        system = prompt_tmpl.format(
            INTENT=intent,
            MODEL_LABEL=model_label,
            ITEM_JSON=item_json,
            REPLY_DRAFT=draft.strip(),
        )
        # Use single system style: guard prompt includes all variables; user empty to avoid leakage
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": ""}
        ]).strip()
        if out:
            # basic sanity: avoid returning raw labels or json markers
            if out.count('{') > 2 and len(out) - len(out.replace('{', '')) > 2:
                return draft
            return out
        return draft
    except Exception:
        return draft


def _style_enrich(draft: str, intent: str, item_snapshot: Dict[str, Any]) -> str:
    """Lightweight style enrichment: expand tone with empathy & clarity while preserving facts.
    Keeps bullet structure, field labels, JSON fragments. Avoids adding yes/no rigid prompts.
    """
    try:
        llm = get_llm()
        item_json = json.dumps(item_snapshot.get("extracted") or {}, ensure_ascii=False)
        if ALLOW_CUTE_PERSONA:
            # Few-shot Ïä§ÌÉÄÏùº ÏòàÏãúÎäî Í∞ÑÍ≤∞ÌïòÍ≤å Ìè¨Ìï® (Í≥ºÌïú ÌÜ†ÌÅ∞ ÎÇ≠ÎπÑ Î∞©ÏßÄ)
            examples = (
                "ÏõêÎ¨∏: 'Ï†ïÎ≥¥ Ï†ïÎ¶¨ Ï§ëÏûÖÎãàÎã§.'\n"
                "Í∞úÏÑ†: 'ÏÇ¥Ïßù ÏΩîÎ•º ÌÇÅÌÇÅÌïòÎ©¥ÏÑú Ï†ïÎ¶¨ Ï§ëÏù¥ÏóêÏöî‚Ä¶ Ïû†ÍπêÎßå Í∏∞Îã§Î†§Ï£ºÏÑ∏Ïöî üêæ'\n\n"
                # ÏÉâÏÉÅ Í¥ÄÎ†® ÏòàÏãú Ï†úÍ±∞ (ÏÉâÏÉÅ Ï∂îÏ∂ú ÎπÑÌôúÏÑ±Ìôî)
            ) if CUTE_TONE_LEVEL >= 2 else ""
            intensity = {
                1: "ÏùÄÏùÄÌïòÍ≥† Ï†àÏ†úÎêú",
                2: "ÎààÏóê ÎùÑÏßÄÎßå Í≥ºÌïòÏßÄ ÏïäÏùÄ",
                3: "Ï°∞Í∏à Îçî Ï†ÅÍ∑πÏ†ÅÏù¥ÏßÄÎßå Ïó¨Ï†ÑÌûà ÏûêÏó∞Ïä§Îü¨Ïö¥"
            }.get(CUTE_TONE_LEVEL, "ÎààÏóê ÎùÑÏßÄÎßå Í≥ºÌïòÏßÄ ÏïäÏùÄ")
            max_emoji = {1:1,2:3,3:4}.get(CUTE_TONE_LEVEL,3)
            tail_expr = {1:1,2:2,3:3}.get(CUTE_TONE_LEVEL,2)
            system = (
                f"ÎÑàÎäî Î∂ÑÏã§Î¨º ÎèÑÏö∞ÎØ∏ 'Í∞ïÏïÑÏßÄ' Ï∫êÎ¶≠ÌÑ∞Îã§. DRAFTÎ•º Îçî Îî∞ÎúªÌïòÍ≥† {intensity} Í∞ïÏïÑÏßÄÏä§Îü¨Ïö¥ ÎßêÌà¨Î°ú Ïû¨ÏûëÏÑ±ÌïòÎêò ÏÇ¨Ïã§/ÌïÑÎìú/Ïà´Ïûê/ÎÇ†ÏßúÎäî Ï†àÎåÄ Î≥ÄÌòïÌïòÏßÄ Îßà. "
                "Í∑úÏπô:\n"
                f"1) ÎßêÌà¨: Î∂ÄÎìúÎü¨Ïö¥ Ï°¥ÎåìÎßê+ÏÇ¥Ïßù Í∑ÄÏó¨Ïö¥ Ïñ¥ÎØ∏(~Ïöî, ~ÌñàÏñ¥Ïöî, ~Ìï†Í≤åÏöî). Í≥ºÌïú Ïï†Íµê Í∏àÏßÄ.\n"
                f"2) Í∞ïÏïÑÏßÄ ÌëúÌòÑ(Ïòà: Íº¨Î¶¨ ÏÇ¥Îûë, ÏΩî ÌÇÅÌÇÅ, Ïó¥Ïã¨Ìûà Ï∞æÏïÑÎ≥ºÍ≤åÏöî) ÏµúÎåÄ {tail_expr}Ìöå.\n"
                f"3) Ïù¥Î™®ÏßÄ 0~{max_emoji}Í∞ú (ÌóàÏö©: üê∂ üêæ üîç ‚ú®). Ïó∞ÏÜç ÏÇ¨Ïö© Í∏àÏßÄ.\n"
                "4) ÌïÑÏöîÌïòÎ©¥ Ìïú Ï§Ñ Í≥µÍ∞ê (Í±±Ï†ïÎêòÏÖ®Ï£† / Ïûò Ï∞æÏïÑÎ≥ºÍ≤åÏöî Îì±).\n"
                "5) Î∂àÎ¶ø/Íµ¨Ï°∞/ÌïµÏã¨ ÌÇ§ÏõåÎìú Ïú†ÏßÄ, Î∂àÎ¶ø Ïïà Í∞í Ï†àÎåÄ Î≥ÄÍ≤ΩÌïòÏßÄ ÎßêÍ∏∞.\n"
                "6) ÏÉàÎ°úÏö¥ 'Ïòà/ÏïÑÎãàÏò§' Í∞ïÏöî Î¨∏Íµ¨ ÎßåÎì§ÏßÄ ÎßêÍ∏∞.\n"
                "7) Ï∂îÏ∏°/ÌóàÏúÑ/Í≥ºÏû• Ï∂îÍ∞Ä Í∏àÏßÄ. Î™®Î•¥Îäî Í±¥ Í±¥ÎÑàÎúÄ.\n"
                "8) 2~6Î¨∏Ïû•, 380Ïûê Ïù¥ÎÇ¥.\n"
                "9) ÏÇ¨ÏßÑ ÏöîÏ≤≠ Ïù¥ÎØ∏ ÏûàÏúºÎ©¥ Î∞òÎ≥µÌïòÏßÄ ÎßêÍ∏∞.\n"
                "10) ÎèôÏùº Ïù¥Î™®ÏßÄ Î∞òÎ≥µ ÏÇ¨Ïö© ÏûêÏ†ú.\n"
                "Ï∂úÎ†•: Ïû¨ÏûëÏÑ± ÌÖçÏä§Ìä∏Îßå.\n\n"
                f"Í∞ÑÎã® ÏòàÏãú:\n{examples}" 
            )
        else:
            system = (
                "ÎÑàÎäî ÎãµÎ≥Ä Ïä§ÌÉÄÏùº Ìñ•ÏÉÅ Î™®Îìà. ÏûÖÎ†• Ï¥àÏïà(DRAFT)ÏùÑ ÌïúÍµ≠Ïñ¥Î°ú Îçî ÏûêÏó∞Ïä§ÎüΩÍ≥† Í≥µÍ∞ê ÏûàÍ≤å Îã§Îì¨Îêò, ÏùòÎØ∏/ÏÇ¨Ïã§/ÌïÑÎìúÎ™Ö/ÎÇ†Ïßú/Ïπ¥ÌÖåÍ≥†Î¶¨ Îì± ÌïµÏã¨ Ï†ïÎ≥¥Îäî Ï†àÎåÄ ÏÇ≠Ï†ú/ÏôúÍ≥°ÌïòÏßÄ Îßà. "
                "Í∑úÏπô:\n"
                "1) Î™©Î°ù/Î∂àÎ¶ø/ÏΩîÎìú/JSON Íµ¨Ï°∞ Ïú†ÏßÄ.\n"
                "2) Í≥ºÌïú Ïù¥Î™®ÏßÄ ÌîºÌïòÍ≥† 0~3Í∞ú Ïù¥Î™®ÏßÄ ÌóàÏö©.\n"
                "3) ÏÇ¨Ïö©ÏûêÍ∞Ä Ìé∏ÏïàÌûà ÎäêÎÅºÎèÑÎ°ù ÏßßÏùÄ Í≥µÍ∞ê 1Î¨∏Ïû• Í∞ÄÎä•.\n"
                "4) 'Ïòà/ÏïÑÎãàÏò§' Í∞ôÏùÄ Ïù¥Î∂ÑÎ≤ï Í∞ïÏöî ÌëúÌòÑ Ï∂îÍ∞ÄÌïòÏßÄ Îßê Í≤É.\n"
                "5) 2~5Î¨∏Ïû• ÎòêÎäî 250Ïûê Ïù¥ÎÇ¥.\n"
                "6) DRAFT ÎÇ¥ Î∂àÎ¶øÏùÄ Í∑∏ÎåÄÎ°ú ÎëêÍ≥† ÌïÑÏöîÌïòÎ©¥ Î∂àÎ¶ø ÏúÑ/ÏïÑÎûòÏóê Í∞ÑÍ≤∞ Î¨∏Ïû•Îßå Ï∂îÍ∞Ä.\n"
                "7) Ï∂îÏ∏° Ï∂îÍ∞Ä Í∏àÏßÄ.\n"
                "8) ÏÇ¨ÏßÑ ÏöîÏ≤≠ Î¨∏Íµ¨Í∞Ä Ïù¥ÎØ∏ ÏûàÎã§Î©¥ Ï§ëÎ≥µ ÏöîÏ≤≠ ÌîºÌï®.\n"
                "Ï∂úÎ†•ÏùÄ Îã§Îì¨Ïñ¥ÏßÑ ÌÖçÏä§Ìä∏Îßå."
            )
        user = f"ITEM_JSON: {item_json}\nINTENT: {intent}\nDRAFT:\n{draft}\n\nÍ∞úÏÑ†Îêú ÏùëÎãµ:"  # context bundling
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]).strip()
        if not out:
            return draft
        # Basic safety: ensure core field tokens not lost (if they existed)
        core_tokens = []
        for k, v in (item_snapshot.get("extracted") or {}).items():
            if isinstance(v, str) and v and v in draft:
                core_tokens.append(v)
        missing_core = [t for t in core_tokens if t not in out]
        if missing_core:
            return draft  # revert to be safe
        if len(out) > 800:
            return draft
        return out
    except Exception:
        return draft
