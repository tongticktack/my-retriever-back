"""Extraction controller orchestrating friendly persona vs structured JSON extraction.

Responsibilities:
- Decide whether to run structured extraction LLM (JSON only) based on new user input & state.
- Run fast rule extraction first, then (conditionally) strict JSON LLM extraction and merge.
- Validate & normalize fields (delegates to lost_item_extractor._validate where possible).
- Determine next conversational action (ask missing field, disambiguate, confirm, or pass-through).
- Maintain per-item stages: collecting -> ready -> confirmed.
"""
from __future__ import annotations
from typing import Dict, Any, Tuple, List, Optional
import re
import json
from datetime import datetime, timedelta
from pathlib import Path

from app.services import lost_item_extractor as li_ext
from app.domain import lost_item_schema as schema
from app.services.llm_providers import get_llm
from config import settings

DATE_ISO_RE = re.compile(r"20\d{2}-\d{2}-\d{2}$")

PERSONA_PREFIXES: List[str] = []  # deprecated; kept for backward compatibility

# Intent keyword heuristics
SHORT_GREETING_TOKENS = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello", "ã…Žã…‡"]
# Identity / capability inquiry patterns considered friendly opening, not off-topic
INTRO_PATTERNS = [
    r"ë„ˆ.*ëˆ„êµ¬", r"ë„ˆ.*ë­í•˜ëŠ”", r"ë„Œ ëˆ„êµ¬", r"ëˆ„êµ¬ì„¸ìš”", r"ì •ì²´ì„±", r"ì—­í• ì´ ë­", r"ë¬´ìŠ¨ ì—­í• ", r"ë¬´ì—‡ì„ í•  ìˆ˜", r"ë­ í•  ìˆ˜", r"ë¬´ìŠ¨ ì¼ì„", r"í•  ìˆ˜ ìžˆì–´\??"
]
INTENT_LABELS = ["greeting", "policy", "item", "confirm", "cancel", "other"]
_INTENT_CACHE: Dict[str, str] = {}
_INTENT_CACHE_MAX = 500

# Toggle for final response guard LLM (lightweight style/safety polish)
ENABLE_RESPONSE_GUARD = True
# Optional stylistic enrichment (makes baseline functional message warmer & empathetic)
ENABLE_STYLE_ENRICH = True
# Allow light cute persona tone (dog helper) in enrichment
ALLOW_CUTE_PERSONA = True
# 1=ì€ì€, 2=í‘œì¤€ ê·€ì—¬ì›€, 3=ê°•ì¡° (ê³¼ìž‰ ë°©ì§€ ë¡œì§ ìœ ì§€)
CUTE_TONE_LEVEL = 2
# PROMPT_VERSION now driven by environment via config.Settings (PROMPT_VERSION env var)
PROMPT_VERSION = getattr(settings, 'PROMPT_VERSION', 1)

# Vision confidence threshold for optional fields (brand/material/pattern ...)
VISION_OPTIONAL_CONF_THRESHOLD = 0.55
VISION_AUTO_OVERRIDE_THRESHOLD = 0.85  # high-confidence vision override threshold for critical fields

def _persona_wrap(prompt: str, kind: str) -> str:
    # Neutral passthrough; style handled by higher-level system prompt / guard.
    return prompt


def _classify_intent_rule(text: str) -> str:
    t = text.strip().lower()
    if not t:
        return "other"
    # short pure greeting only OR identity/intro queries
    if any(t == g.lower() or (t.startswith(g.lower()) and len(t) <= len(g)+2) for g in SHORT_GREETING_TOKENS):
        return "greeting"
    for pat in INTRO_PATTERNS:
        try:
            if re.search(pat, t):
                return "greeting"
        except Exception:
            continue
    return "other"


def _intent_cache_key(text: str) -> str:
    return text.strip().lower()[:200]


def _evict_if_needed():
    if len(_INTENT_CACHE) > _INTENT_CACHE_MAX:
        # naive eviction: drop first 50
        for i, k in enumerate(list(_INTENT_CACHE.keys())):
            del _INTENT_CACHE[k]
            if i >= 49:
                break


def _load_intent_prompt() -> str:
    fname = f"app/prompts/intent_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _classify_intent_llm(text: str) -> str:
    llm = get_llm()
    system = _load_intent_prompt() or (
        "ë„ˆëŠ” ì¸í…íŠ¸ ë¶„ë¥˜ê¸°. greeting|policy|item|confirm|cancel|other ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥."
    )
    user = f"ë©”ì‹œì§€: {text.strip()}\nìž˜ ìƒê°í•˜ê³  ê°€ìž¥ ì•Œë§žì€ í•˜ë‚˜ì˜ ë ˆì´ë¸”ë§Œ ì¶œë ¥."  # thinking nudge
    try:
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]).strip().lower()
    except Exception:
        return "other"
    token = re.split(r"[^a-zê°€-íž£]+", out)[0] if out else "other"
    if token not in INTENT_LABELS:
        # simple containment fallback
        for lbl in INTENT_LABELS:
            if lbl in out:
                token = lbl
                break
    if token not in INTENT_LABELS:
        token = "other"
    return token


def classify_intent(text: str) -> Tuple[str, str]:
    """Return (intent, source) using rule fast-path + LLM fallback + cache."""
    key = _intent_cache_key(text)
    if key in _INTENT_CACHE:
        return _INTENT_CACHE[key], "cache"
    # confirm/cancel tokens quick
    low = key
    # Removed hard token shortcut for confirm/cancel to let LLM handle nuanced affirmation/negation.
    rule = _classify_intent_rule(text)
    if rule in {"greeting"}:
        _INTENT_CACHE[key] = rule; return rule, "rule"
    # LLM fallback for item vs other nuance
    llm_intent = _classify_intent_llm(text)
    _INTENT_CACHE[key] = llm_intent
    _evict_if_needed()
    return llm_intent, "llm"


def _rule_extract(text: str) -> Dict[str, str]:
    # Directly use internal rule analyzer (safe for controlled import)
    try:
        data = li_ext._analyze_rule(text)  # type: ignore[attr-defined]
    except Exception:
        data = {}
    return data


def _load_extraction_prompt() -> str:
    fname = f"app/prompts/extraction_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""

def _load_multimodal_prompt() -> str:
    # Prefer newest v4 taxonomy-aware prompt; fallback to v3.
    for fname in ["app/prompts/multimodal_extraction_v4.txt", "app/prompts/multimodal_extraction_v3.txt"]:
        p = Path(fname)
        if p.exists():
            try:
                return p.read_text(encoding="utf-8")
            except Exception:
                continue
    return ""


def _strict_llm_extract(user_text: str, current: Dict[str, str], image_urls: Optional[List[str]] = None) -> Dict[str, Any]:  # pragma: no cover (LLM)
    """LLM JSON-only structured extraction using external prompt file with retry & light repair."""
    llm = get_llm()
    today = datetime.now().date().isoformat()
    # dynamic few-shot date replacements
    try:
        yesterday = (datetime.now().date() - timedelta(days=1)).isoformat()
        d3 = (datetime.now().date() - timedelta(days=3)).isoformat()
    except Exception:
        yesterday = today
        d3 = today
    # Choose prompt: multimodal if images present else text-only
    if image_urls:
        prompt_tmpl = _load_multimodal_prompt() or _load_extraction_prompt()
    else:
        prompt_tmpl = _load_extraction_prompt()
    current_json = json.dumps(current, ensure_ascii=False) if current else "{}"
    if prompt_tmpl:
        placeholders = {
            'TODAY': today,
            'YESTERDAY': yesterday,
            'D3': d3,
            'D2': (datetime.now().date() - timedelta(days=2)).isoformat(),
            'D7': (datetime.now().date() - timedelta(days=7)).isoformat(),
            'CATEGORIES': ", ".join(schema.PRIMARY_CATEGORIES),
            'CURRENT_JSON': current_json,
            'USER_TEXT': user_text.strip(),
            'IMAGE_COUNT': str(len(image_urls) if image_urls else 0),
        }
        system = prompt_tmpl
        for k, v in placeholders.items():
            system = system.replace('{'+k+'}', v)
    else:
        system = "ë‹¨ì¼ JSON {category, subcategory, color, lost_date, region}; ì—†ëŠ” ê°’ í‚¤ ìƒëžµ; today=" + today
    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": system}
    ]
    # Vision ì§€ì› (OpenAI 4o-mini ë“±) - provider ê°€ openai ì´ê³  ì´ë¯¸ì§€ URL ì´ ìžˆìœ¼ë©´ ë©€í‹°ëª¨ë‹¬ parts êµ¬ì„±
    llm_name = getattr(llm, 'name', '')
    if image_urls and llm_name == 'openai':
        parts: List[Dict[str, Any]] = []
        if user_text.strip():
            parts.append({"type": "text", "text": user_text.strip()})
        for url in image_urls[:3]:  # safety cap
            parts.append({"type": "image_url", "image_url": {"url": url}})
        # ì´ë¯¸ì§€ë§Œ ìžˆëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ìµœì†Œ í•œê°œì˜ text guidance ì¶”ê°€ (ì¶”ì¶œ ì§€ì‹œ)
        if not user_text.strip():
            parts.insert(0, {"type": "text", "text": "ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ë¶„ì‹¤ë¬¼ ì •ë³´ë¥¼ category, subcategory, color, lost_date(ëª¨ë¥´ë©´ ë¹ˆì¹¸), region(ëª¨ë¥´ë©´ ë¹ˆì¹¸) JSON ì¶”ì¶œ"})
        messages.append({"role": "user", "content": parts})
    else:
        messages.append({"role": "user", "content": user_text.strip()})

    def _attempt_parse(raw: str) -> Dict[str, str] | None:
        snippet = raw.strip()
        if '```' in snippet:
            parts = snippet.split('```')
            # choose middle part if possible
            for p in parts:
                if '{' in p and '}' in p:
                    snippet = p; break
        snippet = snippet.strip()
        snippet = re.sub(r'^json\n', '', snippet, flags=re.IGNORECASE).strip()
        # direct parse
        try:
            parsed = json.loads(snippet)
            if isinstance(parsed, dict):
                return parsed
        except Exception:
            pass
        # salvage braces
        m = re.search(r"\{.*?\}", raw, flags=re.DOTALL)
        if m:
            try:
                parsed = json.loads(m.group(0))
                if isinstance(parsed, dict):
                    return parsed
            except Exception:
                pass
        return None

    def _clean(parsed: Dict[str, Any]) -> Dict[str, Any]:
        cleaned: Dict[str, Any] = {}
        scalar_keys = ["category", "subcategory", "color", "lost_date", "region", "brand", "material", "pattern"]
        for k in scalar_keys:
            v = parsed.get(k)
            if isinstance(v, str):
                t = v.strip()
                if t:
                    cleaned[k] = t[:50]
        # Arrays
        def _norm_list(vals, limit):
            out = []
            if isinstance(vals, list):
                for x in vals:
                    if not isinstance(x, str):
                        continue
                    s = x.strip()
                    if not s:
                        continue
                    if len(s) > 25:
                        s = s[:25]
                    if s not in out:
                        out.append(s)
                    if len(out) >= limit:
                        break
            return out
        nf = _norm_list(parsed.get('notable_features'), 5)
        if nf:
            cleaned['notable_features'] = nf
        ts = _norm_list(parsed.get('text_snippets'), 4)
        # rudimentary PII filter: remove email-like / long digit sequences
        filtered_ts = []
        for s in ts:
            if '@' in s:
                continue
            if sum(c.isdigit() for c in s) >= 6:
                continue
            filtered_ts.append(s)
        if filtered_ts:
            cleaned['text_snippets'] = filtered_ts
        # confidences
        confidences = parsed.get('confidences')
        kept_conf: Dict[str, float] = {}
        if isinstance(confidences, dict):
            for k, v in confidences.items():
                try:
                    f = float(v)
                except Exception:
                    continue
                if f < 0 or f > 1:
                    continue
                kept_conf[k] = round(f, 2)
        # Apply threshold removal for optional fields
        for opt_key in ["brand", "material", "pattern"]:
            if opt_key in cleaned:
                conf_v = kept_conf.get(opt_key)
                if conf_v is not None and conf_v < VISION_OPTIONAL_CONF_THRESHOLD:
                    # drop field & its confidence entry
                    cleaned.pop(opt_key, None)
                    kept_conf.pop(opt_key, None)
        if kept_conf:
            cleaned['confidences'] = kept_conf
        return cleaned

    raw = ""
    for attempt in range(2):  # one retry if first malformed
        try:
            raw = llm.generate(messages).strip()
        except Exception:
            return {}
        parsed = _attempt_parse(raw)
        if parsed is not None:
            return _clean(parsed)
        # repair hint for retry: force user to output ONLY JSON
        messages.append({"role": "user", "content": "JSON í˜•ì‹ ì˜¤ë¥˜. ìˆœìˆ˜ JSON ê°ì²´ë§Œ ë‹¤ì‹œ ì¶œë ¥."})
    return {}


def _merge_extracted(base: Dict[str, str], new: Dict[str, str]) -> Dict[str, str]:
    for k, v in new.items():
        if k not in base:
            base[k] = v
    return base


# _has_ambiguity: removed (was unused after logic refactor)


CONFIRM_TOKENS: set[str] = set()  # deprecated
CANCEL_TOKENS: set[str] = set()
OFF_TOPIC_KEYWORDS = [
    "ë‚ ì”¨", "ì£¼ì‹", "íˆ¬ìž", "ì½”ì¸", "ì•”í˜¸í™”í", "ê±´ê°•", "ë³‘ì›", "ì§„ë£Œ", "ì‹ë‹¨", "ì•„ì¹¨ì— ë­", "ìš”ë¦¬", "ë ˆì‹œí”¼",
    "ìš´ë™", "ë‹¤ì´ì–´íŠ¸", "ê³µë¶€", "ì‹œí—˜", "ì—¬í–‰", "í˜¸í…”"
]
SOFT_LOCK_THRESHOLD = 3


def should_run_extraction(item: Dict[str, Any], user_text: str) -> bool:
    """Always run extraction in collecting/ready unless the message is a pure confirm/cancel token.
    This favors accuracy over minimal LLM calls.
    """
    stage = item.get("stage")
    if stage not in {"collecting", "ready"}:
        return False
    # Always run extraction; confirmation now inferred at higher layer (LLM intent), not by raw tokens.
    _log_metric('extraction.trigger', reason='always')
    return True


def process_message(user_text: str, lost_state: Dict[str, Any], start_new: bool, image_urls: Optional[List[str]] = None) -> Tuple[str | None, Dict[str, Any], str, Dict[str, Any] | None]:
    """Process user message relative to lost item flow.
    Returns (assistant_reply_or_none, updated_lost_state, model_label, active_item_snapshot).
    If assistant_reply_or_none is None the caller may proceed with general LLM chat.
    """
    if lost_state is None:
        lost_state = {"items": [], "active_index": None}

    if start_new or not lost_state.get("items"):
        lost_state["items"].append({"extracted": {}, "missing": li_ext.compute_missing({}), "stage": "collecting"})
        lost_state["active_index"] = len(lost_state["items"]) - 1

    idx = lost_state.get("active_index")
    if idx is None:
        return None, lost_state, "lost-item-flow.v2", None
    current = lost_state["items"][idx]
    user_lower = user_text.strip().lower()

    # Intent detection BEFORE extraction (greeting/policy may bypass item flow)
    intent, intent_source = classify_intent(user_text)
    _log_metric('intent.classified', intent=intent, source=intent_source)
    # Soft-lock handling: if previously locked and user still off-topic, short circuit
    if lost_state.get("soft_lock") and intent not in {"item", "greeting"}:
        reply = _persona_wrap(
            "ì„œë¹„ìŠ¤ê°€ ë¶„ì‹¤ë¬¼ íšŒìˆ˜ ì§€ì› ëª¨ë“œë¡œ ìž ì‹œ ìž ê²¼ì–´ìš”. ë¶„ì‹¤í•œ ë¬¼ê±´ì˜ ì¢…ë¥˜, ìž¥ì†Œ, ë‚ ì§œ ì¤‘ í•˜ë‚˜ë¼ë„ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ì‹œ ë„ì™€ë“œë¦´ ìˆ˜ ìžˆì–´ìš”.",
            'ask'
        )
        return reply, lost_state, "intent:locked", _snapshot(idx, current)
    # Allow greeting to release soft lock naturally
    if lost_state.get("soft_lock") and intent == 'greeting':
        lost_state.pop("soft_lock", None)

    if intent in {"greeting", "policy", "other"}:
        if intent == "greeting":
            # Pass through: allow general LLM (system prompt) to craft intro instead of fixed template
            return None, lost_state, "intent:greeting-pass", _snapshot(idx, current)
        if intent == "policy":
            reply = (
                "ë¶„ì‹¤ë¬¼ì€ ë°œê²¬ í›„ ê²½ì°°/ìœ ì‹¤ë¬¼ ì„¼í„°ì— ì¸ê³„ë˜ë©´ í†µìƒ ì¼ì • ê¸°ê°„(ì˜ˆ: 6ê°œì›” ì „í›„, í’ˆëª©/ë²•ê·œì— ë”°ë¼ ì°¨ì´) ë³´ê´€ í›„ ì²˜ë¦¬ë©ë‹ˆë‹¤. "
                "ì •í™•í•œ ìµœì‹  ë³´ê´€ ê¸°ê°„ì€ ê´€í•  ê²½ì°°ì²­/ì§€ìžì²´ ê³µì§€ ë˜ëŠ” ê³µì‹ ìœ ì‹¤ë¬¼ í†µí•©í¬í„¸ì„ í™•ì¸í•´ ì£¼ì„¸ìš”. \n"
                "ë¬¼ê±´ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë°”ë¡œ êµ¬ì¡°í™”í•´ì„œ ê²€ìƒ‰ ì¤€ë¹„ë¥¼ ë„ì™€ë“œë¦´ê²Œìš”."
            )
            return _persona_wrap(reply, 'ask'), lost_state, "intent:policy", _snapshot(idx, current)
        # other (off-topic): block general chat & gently/firmly redirect
        lowered = user_text.lower()
        keyword_hit = next((kw for kw in OFF_TOPIC_KEYWORDS if kw in lowered), None)
        count = lost_state.get("off_topic_count", 0) + 1
        lost_state["off_topic_count"] = count
        if count > SOFT_LOCK_THRESHOLD:
            lost_state["soft_lock"] = True
            _log_metric('intent.soft_lock', count=count)
            msg = "ë¶„ì‹¤ë¬¼ ì •ë³´ ì—†ì´ ë‹¤ë¥¸ ì£¼ì œ ëŒ€í™”ê°€ ê³„ì†ë˜ì–´ ìž ì‹œ ìž ê¸ˆ ìƒíƒœìž…ë‹ˆë‹¤. ì˜ˆ: 'ì–´ì œ ê°•ë‚¨ì—ì„œ ê²€ì • ë°±íŒ© ìžƒì–´ë²„ë ¸ì–´ìš”' ì²˜ëŸ¼ ì•Œë ¤ì£¼ì‹œë©´ ë‹¤ì‹œ ë„ì™€ë“œë¦´ê²Œìš”."
            return _persona_wrap(msg, 'ask'), lost_state, "intent:redirect-lock", _snapshot(idx, current)
        if keyword_hit:
            msg = f"í•´ë‹¹ ì£¼ì œ(ì˜ˆ: {keyword_hit})ëŠ” ì´ ì„œë¹„ìŠ¤ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ìš”. ë¶„ì‹¤í•œ ë¬¼ê±´ì˜ ì¢…ë¥˜ì™€ ìž¥ì†Œ, ë‚ ì§œ ì¤‘ í•˜ë‚˜ë¼ë„ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”. ì˜ˆ: '3ì¼ ì „ ì‹ ì´Œì—ì„œ íŒŒëž€ íœ´ëŒ€í° ìžƒì–´ë²„ë ¸ì–´ìš”'"
        elif count == 1:
            msg = (
                "ì´ ì„œë¹„ìŠ¤ëŠ” ë¶„ì‹¤ë¬¼ íšŒìˆ˜ ì§€ì›ì— ì§‘ì¤‘í•´ìš”. ì–´ë–¤ ë¬¼ê±´ì„ ì–´ë””ì„œ ì–¸ì œì¯¤ ìžƒì–´ë²„ë ¸ëŠ”ì§€ ì•Œë ¤ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”. "
                "ì˜ˆ: 'ì–´ì œ ê°•ë‚¨ì—ì„œ íŒŒëž€ íœ´ëŒ€í° ìžƒì–´ë²„ë ¸ì–´ìš”'"
            )
        elif count == 2:
            msg = (
                "ë¨¼ì € ë¶„ì‹¤ë¬¼ ê¸°ë³¸ ì •ë³´(ì¹´í…Œê³ ë¦¬/ìƒ‰ìƒ/ë‚ ì§œ/ìž¥ì†Œ)ê°€ í•„ìš”í•´ìš”. ì˜ˆ: 'ì§€ë‚œì£¼ ì‹ ì´Œì—ì„œ ê²€ì • ë°±íŒ© ìžƒì–´ë²„ë ¸ì–´ìš”' ì²˜ëŸ¼ ì•Œë ¤ì£¼ì„¸ìš”." )
        else:
            msg = "ë¶„ì‹¤ë¬¼ ì •ë³´(ì˜ˆ: ì–¸ì œ, ì–´ë””ì„œ, ì–´ë–¤ ë¬¼ê±´)ë¥¼ ìž…ë ¥í•´ì£¼ì…”ì•¼ ê³„ì† ë„ì™€ë“œë¦´ ìˆ˜ ìžˆì–´ìš”."
        return _persona_wrap(msg, 'ask'), lost_state, "intent:redirect", _snapshot(idx, current)

    # confirm/cancel intents outside ready stage â†’ treat politely as guidance
    if intent == 'confirm' and current.get('stage') != 'ready':
        # Provide guidance on missing fields
        missing = current.get('missing') or li_ext.compute_missing(current.get('extracted', {}))
        if missing:
            base = li_ext.build_missing_field_prompt(current.get('extracted', {}), missing)
            return _persona_wrap("ì•„ì§ í™•ì¸ ì „ì´ì—ìš”. " + base, 'ask'), lost_state, 'intent:confirm-misplaced', _snapshot(idx, current)
    if intent == 'cancel' and current.get('stage') != 'ready':
        # Just encourage providing info
        return _persona_wrap("ì§„í–‰ì„ ì·¨ì†Œí•  ë‹¨ê³„ê°€ ì•„ì§ ì•„ë‹ˆì—ìš”. ë¬¼ê±´ ì •ë³´ë¥¼ ì¡°ê¸ˆ ë” ì•Œë ¤ì£¼ì‹¤ëž˜ìš”?", 'ask'), lost_state, 'intent:cancel-misplaced', _snapshot(idx, current)

    # Confirmation handling (only when intent=item)
    if current.get("stage") == "ready" and intent == 'confirm':
        current["stage"] = "confirmed"
        snapshot = _snapshot(idx, current)
        return (
            "ê²€ìƒ‰ ì ˆì°¨ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤. (ê²½ì°°ì²­ API ì—°ë™ ì˜ˆì •) ë‹¤ë¥¸ ë¶„ì‹¤ë¬¼ë„ ê³„ì† ë“±ë¡í•  ìˆ˜ ìžˆì–´ìš”. ìƒˆë¡œ ì‹œìž‘í•˜ë ¤ë©´ 'ìƒˆ ë¬¼ê±´'ì´ë¼ê³  ìž…ë ¥í•˜ì„¸ìš”.",
            lost_state,
            "lost-item-flow.v2",
            snapshot,
        )
    if current.get("stage") == "ready" and intent == 'cancel':
        current["stage"] = "collecting"
        # fall through to ask again

    # If confirmed and user gives more info without start_new trigger -> start a new item
    if current.get("stage") == "confirmed" and not start_new:
        lost_state["items"].append({"extracted": {}, "missing": li_ext.compute_missing({}), "stage": "collecting"})
        lost_state["active_index"] = len(lost_state["items"]) - 1
        idx = lost_state["active_index"]
        current = lost_state["items"][idx]

    # If previously soft-locked and now user provides item intent, unlock
    if lost_state.get("soft_lock") and intent == 'item':
        lost_state.pop("soft_lock", None)
        _log_metric('intent.soft_lock_release')

    # Extraction decision
    prev_extracted = current.get("extracted", {}).copy()
    prev_missing = current.get("missing", [])
    if should_run_extraction(current, user_text):
        extracted = current.get("extracted", {}).copy()
        # Rule first
        rule_part = _rule_extract(user_text)
        _merge_extracted(extracted, rule_part)
        # Always call strict LLM for potential refinement (now vision-aware)
        missing_before = li_ext.compute_missing(extracted)
        _log_metric('extraction.llm_call', missing=len(missing_before), ambiguity='1' if 'lost_date_candidates' in extracted else '0')
        llm_part = _strict_llm_extract(user_text, extracted, image_urls=image_urls)
        prev_keys = set(extracted.keys())
        pre_values = {k: extracted.get(k) for k in ["category", "subcategory", "color"] if extracted.get(k)}
        _merge_extracted(extracted, llm_part)
        # Source tagging + conflict detection
        conflicts = current.get('conflicts') or {}
        vision_conf_map = {}
        if llm_part and isinstance(llm_part, dict):
            conf_dict = llm_part.get('confidences') if isinstance(llm_part.get('confidences'), dict) else {}
            # normalize confidence numbers
            for ck, cv in conf_dict.items():
                try:
                    vision_conf_map[ck] = float(cv)
                except Exception:
                    pass
            sources = current.setdefault('sources', {})
            for k in llm_part.keys():
                if k == 'confidences':
                    continue
                if k not in prev_keys and k not in sources and k in extracted:
                    sources[k] = 'vision' if image_urls else 'llm'
            # Detect conflicts for core fields (text value retained unless override criteria)
            for field in ["category", "subcategory", "color"]:
                if field in llm_part and field in pre_values and llm_part[field] != pre_values[field]:
                    # store conflict structure
                    if field not in conflicts:
                        conflicts[field] = {
                            'text_value': pre_values[field],
                            'vision_value': llm_part[field],
                            'vision_confidence': vision_conf_map.get(field)
                        }
            # Auto override rule (color only or also category/subcategory based on high confidence?)
            for field in ["color", "category", "subcategory"]:
                if field in conflicts:
                    vc = conflicts[field].get('vision_confidence') or 0.0
                    if vc >= VISION_AUTO_OVERRIDE_THRESHOLD:
                        # perform override; keep original as alt_<field>_text
                        original = conflicts[field]['text_value']
                        extracted[field] = conflicts[field]['vision_value']
                        extracted[f'alt_{field}_text'] = original
                        sources[field] = 'vision-override'
                        conflicts[field]['auto_overridden'] = True
                    else:
                        conflicts[field]['auto_overridden'] = False
            if conflicts:
                current['conflicts'] = conflicts
            current['sources'] = sources
        # Validate (will keep candidates if present and no final date)
        extracted = li_ext._validate(extracted)  # type: ignore[attr-defined]
        current["extracted"] = extracted
        current["missing"] = li_ext.compute_missing(extracted)

    # If user provided explicit date resolving candidates
    if "lost_date_candidates" in current.get("extracted", {}):
        token = user_text.strip()
        if DATE_ISO_RE.match(token):
            current["extracted"]["lost_date"] = token
            current["extracted"].pop("lost_date_candidates", None)
            current["missing"] = li_ext.compute_missing(current["extracted"])
        else:
            # maybe user picked one candidate verbatim
            cands = current["extracted"]["lost_date_candidates"].split(",")
            for c in cands:
                if c in user_text:
                    current["extracted"]["lost_date"] = c
                    current["extracted"].pop("lost_date_candidates", None)
                    current["missing"] = li_ext.compute_missing(current["extracted"])
                    break

    # Decide next prompt
    extracted = current.get("extracted", {})
    if "lost_date_candidates" in extracted:
        base = li_ext.build_missing_field_prompt(extracted, current.get("missing", []))
        reply = _persona_wrap(base, 'disambiguate')
        current["stage"] = "collecting"
    else:
        missing = current.get("missing", [])
        if missing:
            # Determine next field
            order = [f for f in ["category", "subcategory", "color", "lost_date", "region"] if f in missing]
            next_field = order[0] if order else None
            # Track ask attempts
            ask_counts = current.setdefault("ask_counts", {})
            changed = extracted.keys() != prev_extracted.keys() or any(extracted.get(k) != prev_extracted.get(k) for k in prev_extracted.keys())
            if next_field:
                if (not changed) and prev_missing == missing:
                    ask_counts[next_field] = ask_counts.get(next_field, 0) + 1
                else:
                    # reset counter when progress made or field changed
                    ask_counts[next_field] = 0
                # Guess after 2 unsuccessful asks (count >=2)
                if ask_counts.get(next_field, 0) >= 2:
                    _log_metric('guess.attempt', field=next_field)
                    guess = _guess_field(extracted, next_field)
                    if guess and guess.upper() != 'UNKNOWN':
                        extracted[next_field] = guess
                        # revalidate & recompute
                        extracted = li_ext._validate(extracted)  # type: ignore[attr-defined]
                        current["extracted"] = extracted
                        current["missing"] = li_ext.compute_missing(extracted)
                        missing = current["missing"]
                        ask_counts[next_field] = 0
                        _log_metric('guess.filled', field=next_field)
            current["stage"] = "collecting"
            base = li_ext.build_missing_field_prompt(extracted, missing)
            reply = _persona_wrap(base, 'ask')
        else:
            if current.get("stage") != "confirmed":
                current["stage"] = "ready"
                reply = _persona_wrap(_build_confirmation_summary(extracted), 'confirm')
                # ì‚¬ì§„ ì²¨ë¶€ ì•ˆë‚´ (ì•„ì§ ì´ë¯¸ì§€ ì—†ê³  í•œ ë²ˆë„ ë¬»ì§€ ì•Šì€ ê²½ìš°)
                if not current.get("media_ids") and not current.get("asked_photo"):
                    reply += "\n\nì¶”ê°€ë¡œ ì‚¬ì§„ì´ ìžˆë‹¤ë©´ ì§€ê¸ˆ ìµœëŒ€ 3ìž¥ê¹Œì§€ ì˜¬ë ¤ì£¼ì„¸ìš”. ì—†ìœ¼ë©´ ê·¸ëƒ¥ ê³„ì† ë§ì”€í•˜ê±°ë‚˜ ê²€ìƒ‰ ì§„í–‰ ì˜ì‚¬ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•´ ì£¼ì„¸ìš”."
                    current["asked_photo"] = True
            else:
                reply = None  # no need to say anything; allow general chat

    snapshot = _snapshot(idx, current)
    # Optional final response guard polishing (only for replies we generated here)
    if reply and ENABLE_STYLE_ENRICH:
        enriched = _style_enrich(reply, intent="item", item_snapshot=snapshot)
        if enriched:
            reply = enriched
    if reply and ENABLE_RESPONSE_GUARD:
        guarded = _guard_response(reply, intent="item", model_label="lost-item-flow.v2", item_snapshot=snapshot)
        if guarded:
            reply = guarded
    return reply, lost_state, "lost-item-flow.v2", snapshot


def _snapshot(idx: int, item: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "index": idx,
        "stage": item.get("stage"),
        "extracted": item.get("extracted"),
        "missing": item.get("missing"),
    }


def _log_metric(event: str, **fields: Any) -> None:
    ts = datetime.now().isoformat()
    compact = ' '.join(f"{k}={v}" for k, v in fields.items())
    print(f"[metric] {ts} {event} {compact}")


def _build_confirmation_summary(extracted: Dict[str, Any]) -> str:
    parts = []
    cat = extracted.get('category'); sub = extracted.get('subcategory')
    if cat and sub: parts.append(f"ì¢…ë¥˜: {cat} ({sub})")
    elif cat: parts.append(f"ì¢…ë¥˜: {cat}")
    col = extracted.get('color');
    if col: parts.append(f"ìƒ‰ìƒ: {col}")
    ld = extracted.get('lost_date');
    if ld: parts.append(f"ë‚ ì§œ: {ld}")
    reg = extracted.get('region');
    if reg: parts.append(f"ìž¥ì†Œ: {reg}")
    if not parts:
        return "ì •ë³´ ì •ë¦¬ ì¤‘ìž…ë‹ˆë‹¤."
    return "í™•ì¸í•´ì£¼ì„¸ìš”:\n" + "\n".join(f"- {p}" for p in parts) + "\nìˆ˜ì •í•  ë‚´ìš©ì´ ìžˆìœ¼ë©´ ë°”ë¡œ ì ì–´ì£¼ì‹œê³ , ê´œì°®ìœ¼ë©´ ê³„ì† ì§„í–‰ ì˜ì‚¬ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”."


def _guess_field(extracted: Dict[str, str], field: str) -> str | None:  # pragma: no cover (LLM)
    """Use LLM to guess a plausible value for a missing field after repeated user non-response.
    Returns guessed string or None/UNKNOWN.
    """
    llm = get_llm()
    context_json = json.dumps(extracted, ensure_ascii=False)
    guidelines = {
        'category': 'ê°€ëŠ¥í•œ ëŒ€ë¶„ë¥˜ ì¤‘ ê°€ìž¥ ê°€ëŠ¥ì„± ë†’ì€ 1ê°œ (ì „ìžê¸°ê¸°/ì˜ë¥˜/ê°€ë°©/ì§€ê°‘/ì•¡ì„¸ì„œë¦¬).',
        'subcategory': 'ì´ë¯¸ categoryê°€ ìžˆë‹¤ë©´ ê·¸ í•˜ìœ„ ì†Œë¶„ë¥˜ ì¤‘ ê°€ìž¥ ê°€ëŠ¥ì„± ë†’ì€ 1ê°œ.',
        'color': 'ì¼ë°˜ì ìœ¼ë¡œ ë§Žì´ ì“°ì´ëŠ” í˜„ì‹¤ì ì¸ ìƒ‰ìƒ 1ê°œ (ê²€ì •/íŒŒëž‘/í°ìƒ‰ ë“±).',
        'lost_date': 'ìµœê·¼ 10ì¼ ì´ë‚´ì˜ ë‚ ì§œ ì¤‘ í•©ë¦¬ì ì¸ 1ê°œ (YYYY-MM-DD). ì§€ë‚˜ì¹˜ê²Œ ìž„ì˜ ëŠë‚Œ í”¼í•˜ê¸°.',
        'region': 'í•œêµ­ ë‚´ ì¼ë°˜ì ì¸ ì§€ëª… 1ê°œ (ì˜ˆ: ê°•ë‚¨, ì‹ ì´Œ, ì„œìš¸). ë„ˆë¬´ ìƒì„¸ ì£¼ì†Œ í”¼í•¨.'
    }
    system = (
        f"ë¶„ì‹¤ë¬¼ ì¶”ë¡  ë³´ì¡°ê¸°. ì£¼ì–´ì§„ partial JSON: {context_json}\n" \
        f"ì¶”ì • ëŒ€ìƒ í•„ë“œ: {field}\n" \
        f"ê·œì¹™: 1) í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ 2) ìžì‹ ì—†ìœ¼ë©´ UNKNOWN 3) ì¶”ê°€ ì„¤ëª… ê¸ˆì§€ 4) ì¶œë ¥ì€ ê°’ë§Œ.\n" \
        f"ê°€ì´ë“œ: {guidelines.get(field,'ê°’ 1ê°œ')}"
    )
    try:
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": "ì¶”ì •ê°’ë§Œ ì¶œë ¥"}
        ]).strip()
    except Exception:
        return None
    # Sanitize multi-line -> first line
    if '\n' in out:
        out = out.split('\n',1)[0].strip()
    # Basic cleanup
    out = out.strip().strip('"').strip()
    if not out:
        return None
    if len(out) > 25:
        return None
    return out


def _load_response_guard_prompt() -> str:
    fname = f"app/prompts/response_guard_v{PROMPT_VERSION}.txt"
    path = Path(fname)
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def _guard_response(draft: str, intent: str, model_label: str, item_snapshot: Dict[str, Any]) -> str:
    """Run lightweight LLM pass to enforce style/safety rules. Fallback to original on failure."""
    try:
        prompt_tmpl = _load_response_guard_prompt()
        if not prompt_tmpl:
            return draft
        llm = get_llm()
        item_json = json.dumps(item_snapshot.get("extracted") or {}, ensure_ascii=False)
        system = prompt_tmpl.format(
            INTENT=intent,
            MODEL_LABEL=model_label,
            ITEM_JSON=item_json,
            REPLY_DRAFT=draft.strip(),
        )
        # Use single system style: guard prompt includes all variables; user empty to avoid leakage
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": ""}
        ]).strip()
        if out:
            # basic sanity: avoid returning raw labels or json markers
            if out.count('{') > 2 and len(out) - len(out.replace('{', '')) > 2:
                return draft
            return out
        return draft
    except Exception:
        return draft


def _style_enrich(draft: str, intent: str, item_snapshot: Dict[str, Any]) -> str:
    """Lightweight style enrichment: expand tone with empathy & clarity while preserving facts.
    Keeps bullet structure, field labels, JSON fragments. Avoids adding yes/no rigid prompts.
    """
    try:
        llm = get_llm()
        item_json = json.dumps(item_snapshot.get("extracted") or {}, ensure_ascii=False)
        if ALLOW_CUTE_PERSONA:
            # Few-shot ìŠ¤íƒ€ì¼ ì˜ˆì‹œëŠ” ê°„ê²°í•˜ê²Œ í¬í•¨ (ê³¼í•œ í† í° ë‚­ë¹„ ë°©ì§€)
            examples = (
                "ì›ë¬¸: 'ì •ë³´ ì •ë¦¬ ì¤‘ìž…ë‹ˆë‹¤.'\n"
                "ê°œì„ : 'ì‚´ì§ ì½”ë¥¼ í‚í‚í•˜ë©´ì„œ ì •ë¦¬ ì¤‘ì´ì—ìš”â€¦ ìž ê¹ë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” ðŸ¾'\n\n"
                "ì›ë¬¸: '- ìƒ‰ìƒ: ê²€ì •'\n"
                "ê°œì„ : '- ìƒ‰ìƒ: ê²€ì • (ì§„í•œ ëŠë‚Œì´ë„¤ìš”!)'\n"
            ) if CUTE_TONE_LEVEL >= 2 else ""
            intensity = {
                1: "ì€ì€í•˜ê³  ì ˆì œëœ",
                2: "ëˆˆì— ë„ì§€ë§Œ ê³¼í•˜ì§€ ì•Šì€",
                3: "ì¡°ê¸ˆ ë” ì ê·¹ì ì´ì§€ë§Œ ì—¬ì „ížˆ ìžì—°ìŠ¤ëŸ¬ìš´"
            }.get(CUTE_TONE_LEVEL, "ëˆˆì— ë„ì§€ë§Œ ê³¼í•˜ì§€ ì•Šì€")
            max_emoji = {1:1,2:3,3:4}.get(CUTE_TONE_LEVEL,3)
            tail_expr = {1:1,2:2,3:3}.get(CUTE_TONE_LEVEL,2)
            system = (
                f"ë„ˆëŠ” ë¶„ì‹¤ë¬¼ ë„ìš°ë¯¸ 'ê°•ì•„ì§€' ìºë¦­í„°ë‹¤. DRAFTë¥¼ ë” ë”°ëœ»í•˜ê³  {intensity} ê°•ì•„ì§€ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ìž¬ìž‘ì„±í•˜ë˜ ì‚¬ì‹¤/í•„ë“œ/ìˆ«ìž/ë‚ ì§œ/ìƒ‰ìƒì€ ì ˆëŒ€ ë³€í˜•í•˜ì§€ ë§ˆ. "
                "ê·œì¹™:\n"
                f"1) ë§íˆ¬: ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§+ì‚´ì§ ê·€ì—¬ìš´ ì–´ë¯¸(~ìš”, ~í–ˆì–´ìš”, ~í• ê²Œìš”). ê³¼í•œ ì• êµ ê¸ˆì§€.\n"
                f"2) ê°•ì•„ì§€ í‘œí˜„(ì˜ˆ: ê¼¬ë¦¬ ì‚´ëž‘, ì½” í‚í‚, ì—´ì‹¬ížˆ ì°¾ì•„ë³¼ê²Œìš”) ìµœëŒ€ {tail_expr}íšŒ.\n"
                f"3) ì´ëª¨ì§€ 0~{max_emoji}ê°œ (í—ˆìš©: ðŸ¶ ðŸ¾ ðŸ” âœ¨). ì—°ì† ì‚¬ìš© ê¸ˆì§€.\n"
                "4) í•„ìš”í•˜ë©´ í•œ ì¤„ ê³µê° (ê±±ì •ë˜ì…¨ì£  / ìž˜ ì°¾ì•„ë³¼ê²Œìš” ë“±).\n"
                "5) ë¶ˆë¦¿/êµ¬ì¡°/í•µì‹¬ í‚¤ì›Œë“œ ìœ ì§€, ë¶ˆë¦¿ ì•ˆ ê°’ ì ˆëŒ€ ë³€ê²½í•˜ì§€ ë§ê¸°.\n"
                "6) ìƒˆë¡œìš´ 'ì˜ˆ/ì•„ë‹ˆì˜¤' ê°•ìš” ë¬¸êµ¬ ë§Œë“¤ì§€ ë§ê¸°.\n"
                "7) ì¶”ì¸¡/í—ˆìœ„/ê³¼ìž¥ ì¶”ê°€ ê¸ˆì§€. ëª¨ë¥´ëŠ” ê±´ ê±´ë„ˆëœ€.\n"
                "8) 2~6ë¬¸ìž¥, 380ìž ì´ë‚´.\n"
                "9) ì‚¬ì§„ ìš”ì²­ ì´ë¯¸ ìžˆìœ¼ë©´ ë°˜ë³µí•˜ì§€ ë§ê¸°.\n"
                "10) ë™ì¼ ì´ëª¨ì§€ ë°˜ë³µ ì‚¬ìš© ìžì œ.\n"
                "ì¶œë ¥: ìž¬ìž‘ì„± í…ìŠ¤íŠ¸ë§Œ.\n\n"
                f"ê°„ë‹¨ ì˜ˆì‹œ:\n{examples}" 
            )
        else:
            system = (
                "ë„ˆëŠ” ë‹µë³€ ìŠ¤íƒ€ì¼ í–¥ìƒ ëª¨ë“ˆ. ìž…ë ¥ ì´ˆì•ˆ(DRAFT)ì„ í•œêµ­ì–´ë¡œ ë” ìžì—°ìŠ¤ëŸ½ê³  ê³µê° ìžˆê²Œ ë‹¤ë“¬ë˜, ì˜ë¯¸/ì‚¬ì‹¤/í•„ë“œëª…/ë‚ ì§œ/ìƒ‰ìƒ/ì¹´í…Œê³ ë¦¬ ë“± í•µì‹¬ ì •ë³´ëŠ” ì ˆëŒ€ ì‚­ì œ/ì™œê³¡í•˜ì§€ ë§ˆ. "
                "ê·œì¹™:\n"
                "1) ëª©ë¡/ë¶ˆë¦¿/ì½”ë“œ/JSON êµ¬ì¡° ìœ ì§€.\n"
                "2) ê³¼í•œ ì´ëª¨ì§€ í”¼í•˜ê³  0~3ê°œ ì´ëª¨ì§€ í—ˆìš©.\n"
                "3) ì‚¬ìš©ìžê°€ íŽ¸ì•ˆížˆ ëŠë¼ë„ë¡ ì§§ì€ ê³µê° 1ë¬¸ìž¥ ê°€ëŠ¥.\n"
                "4) 'ì˜ˆ/ì•„ë‹ˆì˜¤' ê°™ì€ ì´ë¶„ë²• ê°•ìš” í‘œí˜„ ì¶”ê°€í•˜ì§€ ë§ ê²ƒ.\n"
                "5) 2~5ë¬¸ìž¥ ë˜ëŠ” 250ìž ì´ë‚´.\n"
                "6) DRAFT ë‚´ ë¶ˆë¦¿ì€ ê·¸ëŒ€ë¡œ ë‘ê³  í•„ìš”í•˜ë©´ ë¶ˆë¦¿ ìœ„/ì•„ëž˜ì— ê°„ê²° ë¬¸ìž¥ë§Œ ì¶”ê°€.\n"
                "7) ì¶”ì¸¡ ì¶”ê°€ ê¸ˆì§€.\n"
                "8) ì‚¬ì§„ ìš”ì²­ ë¬¸êµ¬ê°€ ì´ë¯¸ ìžˆë‹¤ë©´ ì¤‘ë³µ ìš”ì²­ í”¼í•¨.\n"
                "ì¶œë ¥ì€ ë‹¤ë“¬ì–´ì§„ í…ìŠ¤íŠ¸ë§Œ."
            )
        user = f"ITEM_JSON: {item_json}\nINTENT: {intent}\nDRAFT:\n{draft}\n\nê°œì„ ëœ ì‘ë‹µ:"  # context bundling
        out = llm.generate([
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ]).strip()
        if not out:
            return draft
        # Basic safety: ensure core field tokens not lost (if they existed)
        core_tokens = []
        for k, v in (item_snapshot.get("extracted") or {}).items():
            if isinstance(v, str) and v and v in draft:
                core_tokens.append(v)
        missing_core = [t for t in core_tokens if t not in out]
        if missing_core:
            return draft  # revert to be safe
        if len(out) > 800:
            return draft
        return out
    except Exception:
        return draft
